---
title: 딥러닝이 정확히 뭔지 궁금하지 않아?
layout: post   
categories : [ai]
image : /assets/img/study/deep/ch01/ch1-1.png
description: 딥러닝 study 시작
customexcerpt: 인공 지능에 대한 간결한 정의는 보통의 사람이 수행하는 지능적인 작업을 자동화하기 위한 연구 활동 이라고 한다.
---


작성자 : 박정현

# Ch01. 딥러닝이란?

인공 지능과 머신 러닝, 딥러닝에 대한 필수적인 개념을 소개한다.

* random line to make it work. This will be removed.
{:toc}

## 1.1 인공 지능과 머신 러닝, 딥러닝
----
우선 딥러닝이란 주제를 다루기 이전에 용어에 대해서 정확하게 정의할 필요가 있다.  
인공 지능, 머신 러닝, 딥러닝은 무얼 가리키는 말일까? 답은 아래 그림과 같다.  
![1](/assets/img/study/deep/ch01/ch1-1.png)

### 1.1.1 인공 지능
----
인공 지능은 1956년에 연구 분야로 결정되었습니다. 당시 다트머스 대학교(미국 뉴햄프셔주 해노버에 위치)의 존 매카시라는 교수에 의해 워크숍을 조직했고 기술 발전을 목표로 열심히 연구했습니다.  
인공 지능에 대한 간결한 정의는 **보통의 사람이 수행하는 지능적인 작업을 자동화하기 위한 연구 활동** 이라고 한다.  

사실 아주 오랜 기간 동안 많은 전문가는 프로그래머들이 명시적인 규칙을 충분하게 만들어서 DB에 저장된 지식을 다루면 인간 수준의 인공 지능을 만들 수 있다고 믿었고 실제로 symbolicAI라고 불렸음
> 개인적인 생각으론 symolicAI가 knowledge base(KB)와 관련이 있는 것 같고 KB의 명제들을 formula하게 표현하며 다뤘을 것으로 짐작함.  

symbolicAI가 잘 정의된 논리적인 문제를 푸는 데 적합하다는 것이 증명되었지만, 현재의 데이터 매체들(이미지, 음성, 언어 번역)과 같은 복잡한 관계를 해결하기 위한 규칙을 찾는 것은 어려운 일이다.   
이런 복잡한 관계를 해결하기 위해 새로운 방법의 등장으로 **머신러닝** 이 탄생했다.

### 1.1.2 머신러닝
----
머신러닝과 전통적인 프로그래밍 방식의 차이는 입력으로 어떤 것이 주어지냐라고 생각할 수 있어보인다.  
![2](/assets/img/study/deep/ch01/ch1-2.png)  
컴퓨터가 유용한 작업을 하도록 만드는 일반적인 방법은 프로그래머가 입력 데이터를 적절한 해답으로 바꾸기 위해 따라야 하는 규칙(프로그램)을 작성하는 것이다.  
하지만 머신러닝은 반대로 입력 데이터와 상응하는 해답을 보고 규칙을 찾는다. **즉, 머신러닝 시스템은 명시적으로 프로그램되는 것이 아니라 Training 된다고 볼 수 있다.**  예를 들어, 여행 사진을 태깅(Tagging, 어떤 요건에 맞춰 삽입하는 것)하는 일을 자동화하고 싶다면 사람이 이미 태그해 놓은 다수의 사진 샘플을 시스템에 제공해서(Training) 특정 사진에 태그를 연관시키시 위한 통계적 규칙을 학습할 수 있을 것이다.

> 이때, 사람이 이미 태그해 놓은 다수의 사진을 Labels 이라고 부를 것임

### 1.1.3 데이터에서 표현을 학습하기
----
딥러닝을 정의하고 머신러닝과 차이점을 이해하기 위해 머신러닝에 대해 알 필요가 있다.  
머신러닝은 샘플과 기댓값이 주어졌을 때 데이터 처리 작업을 위한 실행 규칙을 찾는 것이다. 이를 위해 3가지가 필요하다.
1. **입력 데이터 포인트** : 예를 들어 이미지 태깅에 관한 작업이라면 데이터 포인트는 사진이된다.
2. **기대 출력** : 이미지 작업에서 기대하는 출력은 강아지, 고양이 같은 태그다.
3. **알고리즘의 성능을 측정하는 방법** : 알고리즘의 현재 출력과 기대 출력 간의 차이를 결정하기 위해 필요하다. 측정값은 알고리즘의 작동 방식을 교정하기 위한 신호로 다시 FeedBack 된다. 이런 수정 단계를 **Learning** 이라고 한다.  

머신러닝 모델은 입력 데이터를 의미 있는 출력으로 변환한다. 이것이 입력과 출력으로 구성된 샘플로 부터 학습하는 과정이다. **때문에 머신 러닝과 딥러닝의 핵심 문제는 데이터를 의미 있게 변환하는 것이다.** 즉, 기대 출력에 가까워지도록 입력 데이터의 유용한 Repersentation을 Learning 하는 것이다. (표현이라고 부르는데 이유가 있으니 [클릭](https://89douner.tistory.com/339)해서 읽어보자)   

간단한 예시를 보자. x축, y축이 있고 좌표 시스템으로 표현된 데이터 포인트가 아래 그림에 있다.  
![3](/assets/img/study/deep/ch01/ch1-3.png)  
그림을 보면 알 수 있듯 흰색, 빨간색 Point가 있다. 이건 좌표를 입력 받아 빨간색 or 흰색인지 출력하는 알고리즘을 개발하려한다. 다음과 같은 요구사항이 있다고 가정하자.  
- 입력은 포인트의 좌표다
- 기대 출력은 포인트의 색상이다.
- 알고리즘의 성능은 정확히 분류한 포인트의 비율을 사용하여 측정한다.

여기서 우리가 원하는 것은 흰,빨 포인트를 완벽히 구분하는 표현 방법이다.  
![4](/assets/img/study/deep/ch01/ch1-4.png)  

예들면 위 그림에서 3번과 같이 x > 0인 것은 빨간색 포인트이다! 혹은 x < 0인 것은 흰색 포인트다! 라는 Repersentation(표현) 으로 나타낼 수 있다.  
간단한 규칙과 연결된 새로운 표현이 이 분류 문제를 깔끔하게 해결한 것이다.  

> '뭐 이정도는 눈으로 할 수 있지'라고 생각한다면 오산이다. 만약 다양한 종류의 필체에 대해 각 문자의 차이점을 설명하라고 했을 때를 생각해보자!  

머신러닝에서 Learning은 유용한 데이터 표현을 만드는 데이터 변환을 피드백 신호를 바탕으로 자동으로 탐색하는 과정을 말한다. 머신러닝 알고리즘은 일반적으로 변환을 찾기 위한 창의력이 없다. **Hypothesis Space(가설 공간)** 이라는 미리 정의된 연산의 모음들을 자세히 조사하는 것뿐이다. 위 자표 평면 그림에서 가설 공간은 가능한 모든 좌표 변환이 가설 공간이 된다.

**즉, 머신러닝은 가능성 있는 공간을 사전에 정의하고(가설 공간) 피드백 신호의 도움을 받아 입력 데이터에 대한 유용한 변환과 규칙을 찾는 것이다.**

### 1.1.4 딥러닝의 '딥(Deep)'이란?
----
딥러닝은 머신러닝의 특정한 분야로 연속된 Layer에서 점진적으로 의미 있는 표현을 배우는 데 강점이 있으며, 데이터로부터 표현을 학습하는 새로운 방식이다. Deep이라는 단어가 어떤 깊은 통찰을 얻을 수 있다는 것을 의미하지 않는다. 참고로 머신 러닝은 얕은 학습이라고 한다.  

> 어디선가 딥러닝은 인간의 뇌를 모델링해서 신경망을 구성한 것이라고 들어봤을 것이다. 이는 근거가 없는 말이므로 속지말자! 딥러닝은 단지 수학 모델일 뿐이다.   

그럼 딥러닝 알고리즘으로 학습된 표현은 어떻게 나타낼까? 몇 개의 층으로 이뤄진 네트워가 이미지 안의 숫자를 인식하기 위해 이미지를 어떻게 변환하는지 아래 그림에서 나타난다.   
![6](/assets/img/study/deep/ch01/ch1-6.png)  
최종 출력에 대해 점점 더 많은 정보를 가지지만 원본 이미지와는 점점 다른 표현으로 숫자 이미지가 변환된다. 심층 신경망(Deep Neural Network) 을 정보가 연속된 필터를 통과하면서 순도 높게(어떤 작업에 대해 유용하게) 다단계 정보 추출 과정이라 생각할 수 있다. 이게 바로 딥러닝이다.  

### 1.1.5 그림 3개로 딥러닝의 작동 원리 이해하기
---
Layer가 입력 데이터를 처리하는 방식은 일련의 숫자로 이뤄진 가중치(Weight)에 저장되어 있다. 기술적으로 말하면 어떤 층에서 일어나는 변환은 그 층의 가중치를 파라미터(Parameter)로 가지는 함수로 표현된다.  
![7](/assets/img/study/deep/ch01/ch1-7.png)  
학습은 주어진 입력을 정확한 타깃에 매핑하기 위해 신경망의 모든 층에 있는 가중치 값을 찾는 것을 말한다. 하지만 어떤 심층 신경망은 수천만 개의 파라미터를 가지기도 한다.  
> 여담으로 ChatGPT3의 파라미터는 1750억라고 함. 

파라미터가 매우 많은 경우 모든 파라미터의 정확한 값을 찾는 것은 어려운 일로 보인다. 왜냐면 하나의 값을 바꾸면 파라미터 전체에 영향을 미치기 때문이다.  

어떤 것을 조정하려면 먼저 관찰해야 한다. 신경망의 출력을 제어하려면 출력이 기대하는 것보다 얼마나 벗어났는지 측정해야한다(예측값과 원본과 차이를 점수로 계산). 이는 신경망의 **손실 함수(Loss function)** 가 담당하는 일이다. 손실 함수를 **목적 함수(Object function)** 혹은 **비용 함수(Cost function)** 라고 부른다.
![8](/assets/img/study/deep/ch01/ch1-8.png)  
기본적인 딥러닝 방식은 이 점수를 피드백 신호로 사용하여 현재 샘플의 손실 점수가 감소되는 방향으로 가중치 값을 조금씩 수정한다.(아래 그림) 이런 과정을 역전파(Backpropagation) 알고리즘을 구현한 옵티마이저(Optimizer)가 담당한다.  
![9](/assets/img/study/deep/ch01/ch1-9.png)  

초기엔 신경망의 가중치가 랜덤한 값으로 할당되므로 랜덤한 변환을 연속적으로 수행한다. 자연스럽게 출력은 기대한 것과 멀어지고 손실 함수를 통해 얻은 점수는 매우 높을 것이다.(높은게 안좋음) 하지만 신경망이 모든 샘플을 처리하며 정점 조정되며 손실 점수가 감소한다. 이것을 **훈련 반복(Training loop)** 라고 부른다. 

> Bacth size 가 얼마고 Epoch 어쩌고 요런게 훈련 반복의 횟수를 결정 짓는 것! 이런 파라미터를 잘 조정해서 손실 점수가 낮은 최적의 값을 산출함.  
> (다른 챕터에 나옴)
  
### 1.1.6 딥러닝의 성과
----
뭐 말할 것도 없고 그냥 지각에 관련된 작업과 자연어 처리 작업에서 괄목할 만한 성과를 냈다.
- 이미지 분류
- 음성 인식
- 필기 인식
- 기계 번역
- TTS 변환
- 디지털 비서(시리, 빅스비)
- 자율 주행 능력
- ChatGPT


## 1.2 딥러닝 이전: 머신러닝의 간략한 역사
----
전통적인 머신러닝의 자세한 설명은 이 책에서 다루지는 않는다.(범위가 아니래) 그래도 간단하게 소개하고 지금까지의 역사적 배경을 설명한다.

### 1.2.1 확률적 모델링
----
**확률적 모델링(Probabilistic Modeling)** 은 통계학 이론을 데이터 분석에 응용한 것이다. 초창기 머신러닝 형태 중 하나고 요즘도 널리 쓰인다. 대표적으로 **나이브 베이즈(Naive Bayes)**가 있다. 또 **로지스틱 회귀(Logistic regression)** 가 있다. 

> 로지스틱 회귀는 현대 머신러닝의 시작과 같다. 프로그래밍 언어를 배우면 처음에 print("Hello World")출력하기와 비슷한??    
> 사실 로지스틕 회귀는 이름만 회귀지 분류 알고리즘이다.

### 1.2.2 초창기 신경망
----
성공적인 첫 번째 신경망 애플리케이션은 **합성곱 신경망(CNN: Convolutional Neural Network)** 과 역전파를 연결하여 손글씨 숫자 이미지를 분류하는 문제에 적용했다. **LeNet** 이라고 부르는 이 신경망은 우편 봉투의 우편 번호 코드를 자동으로 읽기 위해 1990년대 미국 우편 서비스에 사용했다.  

### 1.2.3 커널 방법
-----
**커널 방법(Kernel method)** 은 분류 알고맂므의 한 종류이다. 그중 서포트 벡터 머신(SVM: Support Vector Machine)이 가장 유명하다. SVM은 두 클래스를 나누는 **결정 경계(decision boundary)**를 찾는 분류 알고리즘이다. 이 결정 경계를 찾는 과정은 두 단계로 구성된다.  

1. 결정 경계가 하나의 초평면(hyperplane)으로 표현될 수 있는 새로운 고차원 표현으로 데이터를 매핑한다.  
    ![10](/assets/img/study/deep/ch01/ch1-10.png)

2. 초평면과 각 클래스의 가장 가까운 데이터 포인트 사이의 거리가 최대가 되는 최선의 결정 경계(하나의 분할 초평면)를 찾는다. 이 단계를 **마진 최대화(Margin maximization)** 라고 부른다. 이렇게 함으로써 결정 경계가 훈련 데이터셋 이외의 새로운 샘플에 잘 일반화 되도록 돕는다.

> SVM에 대해 더 자세한 정보는 구글의 힘을 빌리자.   

SVM이 개발되었을 때 간단한 분류 문제에 대해 최고 수준의 성능을 달성했고 광범위한 이론으로 무장된 몇 안 되는 머신 러닝 방법 중 하나가 되었다. 또 수학적으로 깊게 분석하기 용이하여 이론을 이해하고 설명하기 쉽다. 그래서 SVM이 머신러닝 분야에서 오랫동안 인기를 끌었다.  
하지만 SVM은 대용량의 데이터셋에 확장되기 힘들고 이미지 분류 같은 지각에 관련된 문제에서 좋은 성능을 내지 못했다. SVM은 얕은 학습 방법이기 때문에 지각에 관련된 문제에 SVM을 적용하려면 먼저 수동으로 유용한 표현을 추출해야 하는데 이를 **특성 공학(Feature engineering)** 이라한다. 특성 공학은 매우 어렵고 불안정하다. 

### 1.2.4 결정 트리, 랜덤 포레스트, 그레이디언트 부스팅 머신
----
**결정 트리(Decision Tree)** 는 플로차트(flowchart)와 같은 구조를 가지며 입력 데이터 포인트를 분류하거나 주어진 입력에 대해 출력 값을 예측한다.  
![11](/assets/img/study/deep/ch01/ch1-11.png)  

특히 **랜덤 포레스트(Random Forest)** 알고리즘은 결정 트리 학습에 기초한 것으로 안정적이고 실전에서 유용하다. 서로 다른 결정 트리를 많이 만들고 그 출력을 앙상블(여러 단순한 모델을 결합하여 정확한 모델을 만드는 방법)하는 방법을 사용한다.  

이후 **그레이디언트 부스팅 머신(Gradient boosting machine)** 가 잘 사용되었다. 이 알고리즘은 이전 모델에서 놓친 데이터 포인트를 보완하는 새로운 모델을 반복적으로 훈련함으로써 머신러닝 모델을 향상하는 방법인 **그레이디언트 부스팅(Gradient boosting)** 을 사용한다.

### 1.2.5 딥러닝의 특징
----
딥러닝은 머신러닝에서 가장 중요한 단계인 특성 공학을 완전히 자동화하기 때문에 문제를 더 해결하기 쉽게 만들어 준다.  

머신러닝 기법은 입력 데이터를 고차원 비선형 투영(SVM)이나 결정트리 같은 간단한 변환을 통해 하나 또는 2개의 연속된 표현 공간으로만 변환한다. 하지만 복잡한 문제에 필요한 잘 정제된 표현은 일반적으로 이런 방식으로 얻지 못한다.(사람이 초기 입력 데이터를 여러 방식으로 변환해야함)  

딥러닝은 순차적으로 학습하는 것이 아닌, 동시에 공동으로 학습하게 만든다(병렬 처리). 이런 **공공 특성 학습 능력** 덕분에 자동으로 변화하는 것에 적응하게 된다.  

딥러닝이 데이터로 부터 학습하는 방법에는 두 가지 중요한 특징이 있다.
1. 층(Layer)을 거치면서 점진적으로 더 복잡한 표현이 만들어진다.
2. 이런 점진적인 중간 표현이 공동으로 학습된다.


## 1.3 왜 딥러닝일까? 그리고 왜 지금일까?
----
컴퓨터 비전(이른바 cv라고 부르는 분야)에 대한 딥러닝의 두 가지 핵심 아이디어인 합성곱 신경망과 역전파는 오래전에 소개되었다. 시계열을 위한 딥러닝의 기본적인 LSTM(Long Short-Term Memory) 알고리즘은 1997 년에 개발되었고 그 이후로는 변화가 거의 없다. 그러면 왜 2012년 이후 딥러닝이 부상하게 되었을까? 지난 20년간 어떤 변화가 있었던 걸까?

### 1.3.1 하드웨어
----
시중에 판매되는 CPU는 1990년 ~ 2010년 사이 약 5,000배 빨라졌다고 한다. 그리고 2019년 2,500달러짜리 게임용 GPU(그래픽 카드)인 NVIDIA TITAN RTX는 16테라플롭스(초당 16조 개의 float32연산)의 성능을 제공한다.

이렇게 하드웨어의 성능이 향상됨에 따라 딥러닝 또한 주목 받고 있다.

### 1.3.2 데이터 
----
하드웨어의 성능 향상에 따라 인터넷 성장도 꾸준히 증가했다. 이렇게 인터넷 공간의 수많은 데이터는 핵심 데이터 셋이다. 예를 들어 플리커(Flickr)에서 사용자가 라벨링한 이미지 태그는 컴퓨터 비전의 입장에서는 보물 같은 데이터다. 유튜브의 비디오도 마찬가지 이며, 위키피디어(Wikipedia)는 자연어 처리 분야에 필요한 핵심 데이터셋이다.  

### 1.3.3 알고리즘
----
2009년 ~ 2010년경에 몇 가지 간단하지만 중요한 알고리즘이 개선되면서 그레이디언트를 더 잘 전파되게 만들어 주었고 판도가 변화했다.
- 신경망의 Layer에 더 잘 맞는 **활상화 함수(Activation function)**
- Layer 사전 훈련(pretraning)을 불필요하게 만든 **가중치 초기화(Weight initalization)** 방법
- RMSProp과 Adam 같은 더 좋은 **최적화 방법(최적화 알고리즘)**  

### 1.3.4 과연 지속될 것인가?
----
딥러닝의 현재 상태를 AI의 혁명이라고 정의할 수 있는 몇 가지 특징이 있다.
- **단순함** : 딥러닝은 특성 공학이 필요하지 않아 복잡하고 불안정한 많은 언지니어링 과정을 End-to-End로 훈련시킬 수 있는 모델로 바꾸어 준다.
- **확정성** : 딥러닝은 GPU 또는 TPU에서 쉽게 병렬화할 수 있기 때문에 무어의 법칙 혜택을 크게 볼 수 있다.
- **다용도와 재사용성** : 이전의 많은 머신러닝 방법과는 다르게 딥러닝 모델은 처음부터 다시 시작하지 않고 추가되는 데이터로도 훈련할 수 있다. 대규모 제품에 사용되는 모델에서 아주 중요한 기능은 연속적인 온라인 학습(online learning)을 가능하게 한다.

> 저자는 오늘날 딥러닝 기술을 해결 가능한 모든 문제에 적용할 수 있다는 것에 매우 흥분된다고 한다.   
> 또 '딥러닝 혁명은 여전히 진행 중이며 그 잠재력을 완전히 발휘하려면 수년이 걸릴 것이다.' 라고 주장한다. 
