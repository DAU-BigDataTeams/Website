---
title: 훌륭한 기계 학습 모델 구축을 위한 Feature Engineering 학습하다.
layout: post   
categories : py, kaggle, ML, Data, Data-processing
image : /assets/img/수료증/이수원-DataFeature수료증.png
description: Kaggle의 교육 과정 중 Feature Engineering을 학습하며, 정리하고 번역한 글입니다. 
customexcerpt: 형상이 유용하려면 모형이 학습할 수 있는 대상과 관계가 있어야 한다. 예를 들어, 선형 모델은 선형 관계만 학습할 수 있다. 따라서 선형 모형을 사용할 때, 목표는 형상과 대상의 관계를 선형으로 만들기 위해 형상을 변환하는 것이다.
---

# Feature Engineering
Better features make better models. Discover how to get the most out of your data.

### Welcome to Feature Engineering!

이번 과정에서는 훌륭한 기계 학습 모델을 구축하는 과정에서 가장 중요한 단계 중 하나인 Feature Engineering에 대해 배울 예정이다. 

 

- 상호적인 정보에서 가장 중요한 기능을 결정한다.
- 몇 가지 실제 문제 영역에서 새로운 기능을 개발한다.
- 실제 세계의 문제 도메인을 범주형을 대상 인코딩 한다.
- k-벡터 클러스터링을 사용하여 분할 기능을 생성한다.
- 주 성분 분석을 통해 데이터 세트의 변형을 특징으로 변형한다.

### ****The Goal of Feature Engineering****

Feature Engineering의 목표는 단순히 당면한 문제에 보다 적합한 데이터를 만드는 것이다. 즉, 외부에서 실제로 느끼는 느낌과 더 관련성이 있게 만들려는 시도. 

- 모델의 예측 성능을 향상시킨다.
- 계산 또는 데이터 요구를 줄인다.
- 결과의 해석성을 높인다.

### A Guiding Principle of Feature Engineering | 형상공학의 지도 원리

형상이 유용하려면 모형이 학습할 수 있는 대상과 관계가 있어야 한다. 예를 들어, 선형 모델은 선형 관계만 학습할 수 있다. 따라서 선형 모형을 사용할 때, 목표는 형상과 대상의 관계를 선형으로 만들기 위해 형상을 변환하는 것이다.

여기서 핵심 아이디어는 형상에 적용하는 변환이 본질적으로 모델 자체의 일부가 된다는 것이다. 

예를 들어, 한 변의 길이에서 대지의 제곱 그림 가격을 예측하려고 한다고 가정하자.

선형 모형을 길이에 직접 적합하면 관계가 선형이 아닌 불량한 결과를 얻을 것이다. 

 

![Untitled](https://i.imgur.com/5D1z24N.png)

그러나, ‘면적’을 얻기 위해 길이의 특성을 제곱하면 선형 관계가 형성된다. 특징들의 집합에 면적을 추가하면 이 선형 모델이 포물선을 적합 시킬 수 있다. 즉, 형상의 제곱은 선형 모형에 형상의 제곱을 적합 시킬 수 있는 능력을 부여한 것이다. 

![Untitled](https://i.imgur.com/BLRsYOK.png)

*위 그림을 보면, 왼쪽의 ‘면적’을 기준으로 했을 때가 선형 모델에 더 적합함을 알 수 있다.* 

이를 통해, Feature Engineering에 투자한 시간에 대한 높은 수익률이 가능한 이유를 알 수 있다. 모델이 학습할 수 없는 관계가 무엇이든, 변환을 통해 원하는 모델을 제공할 수 있다. 특징 집합을 개발할 때, **모델이 최상의 성능을 달성하는데 사용할 수 있는 정보가 무엇인지 생각하는 것**이 중요하다. 

*출처 :: [https://www.kaggle.com/code/ryanholbrook/what-is-feature-engineering](https://www.kaggle.com/code/ryanholbrook/what-is-feature-engineering)*

---



### ****Introduction****

해당 과정에서는 **PCA(주성분 분석)** 에 대해 살펴보자. 

클러스터링이 근접성을 기반으로 데이터셋을 분할하는 것과 마찬가지로, PCA도 데이터의 변동을 분할하는 것으로 생각할 수 있다.

 PCA는 데이터에서 중요한 관계를 발견하는데 도움이 되는 훌륭한 도구이며, 더 많은 정보를 제공하는 기능을 만드는데도 사용할 수 있다. 

*(기술 참고: PCA는 일반적으로 표준화 된 데이터에 적용된다. 표준화 된 데이터에서 "변동"은 "상관"을 의미한다. 표준화 되지 않은 데이터의 경우 "변동"은 "공분산"을 의미합니다. 이 과정의 모든 데이터는 PCA를 적용하기 전에 표준화된다.)*



### ****Principal Component Analysis****

전복 데이터 세트에는 수천 개의 태즈메이니아 전복에서 얻은 물리적 측정 값이 있다. 

지금은 껍질의 '높이'와 '지름'이라는 두 가지 특징을 살펴보자.

이 데이터 내에 전복이 서로 다른 경향을 나타내는 "변동 축"이 있다고 상상할 수 있다. 

그림에서 이러한 축은 각 원래 형상에 대해 하나의 축인 데이터의 자연 치수를 따라 수직선으로 표시다.

![Untitled](https://i.imgur.com/rr8NCDy.png)

종종 이러한 변동 축에 이름을 지정할 수 있다.

 더 긴 축을 "크기" 구성 요소라고 할 수 있다. 
 
 작은 높이와 작은 직경(왼쪽 아래)은 큰 높이와 큰 직경(오른쪽 위)과 대조된다. 짧은 축을 "모양" 구성 요소라고 할 수 있다. 
 
 작은 높이와 큰 직경(평평한 모양)은 큰 높이와 작은 직경(원형)과 대조된다.

전복을 '높이'와 '지름'으로 설명하는 대신 '크기'와 '모양'으로 설명할 수도 있다. 

사실 이것은 PCA의 전체 아이디어다. 데이터를 원래 기능으로 설명하는 대신 변동 축으로 설명한다. 

변동 축이 새로운 형상이 된다.

![Untitled](https://i.imgur.com/XQlRD1q.png)


***주요 구성 요소는 형상 공간에서 데이터 세트를 회전 시킴으로써 새로운 형상이 된다.***



PCA가 구성하는 새로운 기능은 사실 원래 기능의 선형 조합(가중 합계)에 불과하다.



~~~py
df["Size"] = 0.707 * X["Height"] + 0.707 * X["Diameter"]
df["Shape"] = 0.707 * X["Height"] - 0.707 * X["Diameter"]
~~~



이러한 새로운 기능을 데이터의 주요 구성 요소라고 한다. 

무게 자체를 적재라고 한다. 

원래 데이터 세트에 있는 기능만큼 많은 주요 구성 요소가 있을 것이다. 

두 개가 아니라 열 개의 기능을 사용했다면 10개의 구성 요소로 끝났을 것이다.

구성 요소의 적재는 기호와 크기를 통해 어떤 변동을 표현하는지 알려준다.


![Untitled](https://i.imgur.com/xWTvqDA.png)


이 적재 표는 크기 구성 요소에서 높이와 지름이 같은 방향(동일한 부호)으로 다양하지만 모양 구성 요소에서는 반대 방향(반대 부호)으로 다양하다는 것을 나타낸다. 

각 성분에서 적재 크기가 모두 동일하므로 형상이 둘 다 동일하게 기여한다.

PCA는 또한 각 성분의 변동량을 알려준다. 

그림을 통해 크기 성분을 따르는 데이터의 변동이 모양 성분을 따르는 것보다 더 많다는 것을 알 수 있다. 

PCA는 각 성분의 설명된 분산 비율을 통해 이를 정확하게 만든다.

***크기는 약 96%를 차지하고 형상은 높이와 지름 사이의 분산의 약 4%를 차지합니다.***


크기 구성 요소는 높이와 지름 사이의 변동 대부분을 포함한다. 

그러나 성분의 분산 양이 예측 변수로서 얼마나 좋은지와 반드시 일치하는 것은 아니며 예측하려는 내용에 따라 다르다.



### ****PCA for Feature Engineering****

Feature Engineering에 PCA를 사용할 수 있는 두 가지 방법이 있다. 

- 첫 번째 방법은 그것을 기술적인 기법으로 사용하는 것이다. 

성분이 변동에 대해 알려주므로 성분에 대한 MI 점수를 계산하고 목표 값을 가장 예측할 수 있는 변동의 종류를 확인할 수 있다.

이를 통해 '높이'와 '지름'의 곱(예: '크기'가 중요한 경우) 또는 '높이'와 '지름'의 비율(모양이 중요한 경우)을 만들 수 있는 형상에 대한 아이디어를 얻을 수 있다. 

점수가 높은 구성 요소 중 하나 이상에서 클러스터링을 시도할 수도 있다.

- 두 번째 방법은 구성 요소 자체를 기능으로 사용하는 것이다. 성분은 데이터의 변동 구조를 직접 노출하기 때문에 원래 특성보다 더 유용할 수 있다. 


- **Dimensionality reduction (차원 축소) : 기능이 고도로 중복된 경우(특히 다중 선형인 경우), PCA는 이중화를 하나 이상의 거의 0에 가까운 분산 구성 요소로 분할한다. 이러한 구성 요소에는 정보가 거의 없거나 거의 포함되어 있지 않기 때문에 해당 구성 요소를 삭제 할 수 있다.**
- **Anomaly detection (이상 탐지) : 원래 형상에서 분명하지 않은 비정상적인 변동이 저분산 성분에 나타나는 경우가 많다. 이러한 구성 요소는 이상 징후 또는 특이치 탐지 작업에서 매우 유용할 수 있다.**
- **Noise reduction (잡음 제거) : 센서 판독값의 집합은 일반적인 배경 노이즈를 공유하는 경우가 많다. PCA는 때때로 노이즈를 그대로 둔 채로 정보적인 신호를 더 적은 수의 기능으로 수집하여 신호 대 잡음 비율을 높일 수 있다.**
- **Decorrelation (장식 관계) : 일부 ML 알고리즘은 상관 관계가 높은 기능으로 어려움을 겪는다. PCA는 상관 관계가 있는 기능을 상관 관계가 없는 구성 요소로 변환하므로 알고리즘 작업이 더 쉬울 수 있다.**

PCA는 기본적으로 데이터의 상관 구조에 직접 액세스할 수 있게 해준다.





<aside>
💡 **PCA 모법 사례** 
PCA를 적용할 때 주의해야 할 몇 가지 사항이 있다. 

- PCA는 연속 수량 또는 카운트와 같은 숫자 기능에서만 작동한다.
- PCA는 규모에 민감하다. PCA를 적용하기 전에 데이터를 표준화 하는 것이 좋다. 그렇지 않은 경우에는 PCA를 사용하는 것은 효율적이지 않다.
- 특이치는 결과에 과도한 영향을 미칠 수 있으므로, 특이치를 제거하거나 제한하는 것을 고려해야 한다.

</aside>


## 6. Taget Encoding 

Boost any categorical feature with this powerful technique.

이 강력한 기술로 모든 범주형 기능을 강화할 수 있다.

### ****Introduction****

앞에 배워 온 Data Featuring은 수치적 특징을 위한 것이었다. 

이번 과정에서 살펴볼 대상인 **인코딩 기술은 범주형 기능에 대한 것**이다. 

이것은 **카테고리를 숫자로 인코딩하는 방법**이다. 예를 들어 One-hot 또는 label 인코딩과 같이 대상을 사용하여 인코딩을 생성한다는 점에서 차이가 있다. 

이를 통해 소위 **supervised feature engineering technique**이라고 한다.



~~~py
# 해당 csv 파일은 Kaggle에 있음을 알립니다.
import pandas as pd

autos = pd.read_csv("../input/fe-course-data/autos.csv")
~~~


### ****Target Encoding****

A **target encoding**is any kind of encoding that replaces a feature's categories with some number derived from the target. 

: 대상 인코딩은 기능의 범주를 대상에서 파생된 숫자로 대체하는 모든 종류의 encoding이다.

예를 들어보자. 간단하고 효과적인 사용 방법으로는 평균과 같이, 그룹 집계를 적용하는 것이 있다. 

자동차 데이터 세트를 사용하여 각 차량의 평균 가격을 계산해보자.


~~~py
autos["make_encoded"] = autos.groupby("make")["price"].transform("mean")

autos[["make", "price", "make_encoded"]].head(10)
~~~


이러한 종류의 대상 인코딩을 **mean encoding(평균 인코딩)**이라고도 한다. 

이진 대상에 적용되는 이를 **bin counting**이라고도 한다. (다른 이름 : likelihood encoding, impact encoding, and leave-one-out encoding.)


### ****Smoothing****

그러나 이와 같은 인코딩은 몇 가지 문제를 제기한다. 

- 첫 번째로는 **‘알 수 없는 범주’** 이다. 

**대상 인코딩은 과적합의 특별한 위험을 초래**하므로, 독립적인 ‘encoding’ 분할에 대해 교육을 받아야 한다. 

이후 분할에 인코딩을 결합하면 인코딩 분할에 없는 범주에 대한 결측값이 padas에 의해 채워진다. 

이러한 누락 값들은 어떻게든 귀속 시켜야 한다. 


- 두 번째는 **‘희귀한 범주’**  이다.

 범주가 데이터 집합에서 몇 번만 발생하는 경우, 해당 그룹에 대해 계산된 통계는 정확하지 않을 수 있다. 
 
 자동차 데이터 세트에서 수은 생성은 한 번만 발생한다. 
 
 우리가 계산한 ‘평균’ 가격은 해당 한 차량 한 대의 가격일 뿐이며, 미래에 볼 수 있는 어떤 수은도 그닥, 대표적이지 않을 가능성이 높다. 
 
 즉, **대상 인코딩은 희귀 범주에서 과적합의 가능성을 높일 수도 있다**.  



이러한 문제에 대한 해결책은 **smoothing(평활화)** 를 추가하는 것이다. 

이 방법은 **범주 내 평균과 전체 평균을 혼합하는 것** 이다. 희귀한 범주는 범주 평균에 대한 가중치가 적은 반면, 누락된 범주는 전체 평균만 가져오게 된다. 


~~~py
encoding = weight * in_category + (1 - weight) * overall
~~~


여기서 weight는 범주 빈도에서 계산된 0과 1 사이의 값이다.



무게 값을 쉽게 확인할 수 있는 방법은 **m-추정치** 를 계산하는 것이다.


~~~py
weight = n / (n + m)
~~~


여기서 **n은 데이터에서 해당 범주가 발생하는 총 횟수** 이다.

 **매개변수 m은 "평활 계수"를 결정** 한다. m 값이 클수록 전체 추정치에 더 많은 가중치가 부여됩니다.

![Untitled](https://i.imgur.com/1uVtQEz.png)

*자동차 데이터 세트에는 쉐보레를 만드는 세 대의 자동차가 있다. m=2.0을 선택한 경우 쉐보레 범주는 평균 쉐보레 가격의 60%에 전체 평균 가격의 40%를 더한 값으로 인코딩다.*


~~~py
chevrolet = 0.6 * 6000.00 + 0.4 * 13285.03
~~~


<aside>
💡 
    **값 형식을 선택할 때 범주의 노이즈를 고려해야 한다**

    차량 가격은 제품마다 크게 다른가? 좋은 견적을 얻으려면 많은 데이터가 필요한가? 그렇다면 더 큰 값의 형태를 선택하는 것이 더 나을 수 있다. 
    
    각 제품의 평균 가격이 비교적 안정적이라면 더 작은 값을 사용해도 괜찮을 수 있다.

</aside>

<aside>
💡 

    **Use Cases for Target Encoding**
    
    1. High-cardinality features : one-hot 인코딩은 너무 많은 기능을 생성할 수 있으며 레이블 인코딩과 같은 대체 기능은 해당 기능에 적합하지 않을 수 있다. 
    
    대상 인코딩은 기능의 가장 중요한 속성인 대상과의 관계를 사용하여 범주에 대한 숫자를 파생한다.

    2. Domain-motivated features : 이전의 학습으로 인해, 특성 행렬의 점수가 좋지 않더라도 범주형 특성이 중요하다고 생각할 수 있다. 
    
    대상 인코딩은 기능의 실제 정보를 표시하는데 도움이 된다.

</aside>

# 해당 POST는 Kaggle 강의 중 'Feature Engineering' 과정의 해석본과 다름 없습니다. 


### 정확한 자료와 실습을 위하여, Kaggle의 해당 강의를 수강하는 것을 추천합니다.   
[Kaggle](https://www.kaggle.com/learn/feature-engineering)
### 작성자의 Notion에 강의 자료와 실습 풀이 내용이 정리되어 있습니다.   
[작성자 Notion](https://www.notion.so/Feature-Engineering-376e30e93d2e460d859f84e2d9714c53)

![1](/assets/img/수료증/이수원-DataFeature수료증.png)