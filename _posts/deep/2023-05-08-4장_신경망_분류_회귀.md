---
title: 케라스 창시자에게 배우는 딥러닝 4장 신경망 시작하기 
layout: post   
categories : 이진분류, 다중분류, 분류, 회귀, 딥러닝
image : ![keras](http://image.yes24.com/goods/112012471/XL)
description: 본 포스트는 케라스 창시자에게 배우는 딥러닝 개정 2판의 4장 내용을 공부한 내용을 담고 있다. 
customexcerpt: 이진 분류, 다중 분류, 회귀 실습하기
---

## 들어가기
<br>
'케라스 창시자에게 배우는 딥러닝 개정 2판'의 4장 신경망 시작하기 : 분류와 회귀를 공부하며 정리한 글이다. 시작하기에 앞서 우리는 2장과 3장에서 딥러닝에 대한 기본 지식들을 공부했다. 이번 장에서는 신경망이 가장많이 사용되는 세 종류의 직업인 *이진 분류*, *다중 분류* 그리고 스칼라 값을 예측하는 *회귀*에 배운 것들을 적용해 볼 예정이다. 실습할 예제는 다음과 같다.

- **이진 분류** : 영화 리뷰를 긍정 또는 부정으로 분류하기
- **다중 분류** : 신문 기사를 토픽으로 분류하기
- **스칼라 회귀** : 부도산 데이터를 바탕으로 주택 가격을 예측하기 

이 예제에서 우리는 'End-to-End' 머신러닝 워크플로(workflow)일 것이다. 

데이터의 전처리, 모델 구조의 기본 원리, 모델 평가에 대해 소개할 예정이다. 

<br>

---

<br>

## 분류와 회귀에서 사용하는 용어 
- **샘플** 또는 **입력** : 모델에 주입될 하나의 데이터 포인트(data point)
- **예측** 또는 **출력** : 모델로부터 나오는 값
- **타깃** : 정답, 외부 데이터 소스에 근거하여 모델이 완벽하게 예측해야 하는 값
- **예측 오차** 또는 **손실값** : 모델의 예측과 타깃 사이의 거리를 측정한 값
- **클래스** : 분류 문제에서 선택할 수 있는 가능한 레이블의 집합. *예를 들어, 고양이와 강아지 사진을 분류 할 때 클래스는 '고양이'와 '강아지' 2개이다.*
- **레이블** : 분류 문제에서 클래스 할당의 구체적인 사례. *예를 들어, 사진 #1234에 '강아지' 클래스가 들어 있다고 표시한다면, '강아지'는 사진 #1234의 레이블이 된다.*
- **참 값(ground-truth)** 또는 **애너테이션(annotation)** : 데이터 셋이 대한 모든 타깃. 일반적으로 사람에 의해 수집된다. 
- **이진 분류** : 각 입력 샘플이 2개의 배타적인 범주로 구분되는 분류 작업
- **다중 분류** : 각 입력 샘플이 여러 개의 레이블에 할당될 수 있는 분류 작업. 예를 들어 하나의 이미지에 고양이와 강아지가 모두 들어 있을 때는 '강아지' 레이블과 '고양이' 레이블을 모두 할당해야 한다. 보통 이미지마다 레이블의 개수는 다르다.
- **스칼라 회귀** : 타깃이 연속적인 스칼라 값인 작업. 주로 주택 가격 예측이 좋은 예시로 사용된다. 각기 다른 타깃 가격이 연속적인 공간을 형성한다. 
- **벡터 회귀** : 타깃이 연속적인 값의 집합인 작업. 예를 들어 연속적인 값으로 이루어진 벡터이다. *예를 들어, 이미지에 있는 경계 상자(bounding box)의 좌표 같은)* 여러 개의 값에 대한 회귀를 한다면 벡터 회귀이다.
- **미니 배치** 또는 **배치** : 모델에 의해 동시에 처리되는 소량의 샘플 묶음(일반적으로 8개에서 128개 사이). 샘플 개수는 GPU의 메모리 할당이 용이하도록 2의 거듭제곱으로 하는 경우가 많다. 훈련할 때 마다 미니 배치마다 한 번씩 모델의 가중치에 적용할 경사 하강법 업데이트 값을 계산한다. 

<br>

---

<p>

# 4.1 영화 리뷰 분류 : 이진 분류 문제 

<br>

**2종 분류(two-class classification)** 또는 **이진 분류(binary classification)**는가장 널리 적용된 머신 러닝 문제이다. 아래 예제에서 리뷰 텍스트를 기반으로 영화 리뷰를 긍정과 부정으로 분류하는 방법을 만들어 볼 예정이다. 

<br>

## 4.1.1 IMDB 데이터셋 
해당 예제에서는 인터넷 영화 데이터베이스(Internet Movie Database)로부터 가져온 양극단의 리뷰 5만 개로 이루어진 IMDB 데이터셋을 사용할 예정이다. 해당 데이터셋은 훈련 데이터 2만 5,000개와 테스트 데이터 2만 5,000개로 나뉘어 있고 각각 50%는 부정, 50%는 긍정 리뷰로 구성되어 있다. 

MNIST 데이터셋처럼 IMDB 데이터셋도 케라스에 포함되어 있다. 이 데이터는 전처리되어 있어 각 리뷰(단어 시퀀스)가 숫자 시퀀스로 변환되어 있다. 여기에서 각 숫자는 사전에 있는 고유한 단어를 나타낸다. 이렇게 전처리된 데이터를 사용하면 모델 구축, 훈련, 평가에 초점을 맞출 수 있다. *11장에서 본격적으로 텍스트 데이터를 처음부터 처리하는 방법을 배운다.*

<br>

```PY
# IMDB 데이터셋 로드하기
from tensorflow.keras.datasets import imdb

(train_data, train_lebels), (test_data, test_lebels) = imdb.load_data(num_words=10000)
```

<br>

num_words=10000 매개변수는 훈련 데이터에서 가장 자주 나타나는 단어 1만 개만 사용하겠다는 의미이다. 
해당 예제에서는 드물게 나타나는 단어는 무시해서 진행한다. 이렇게 하면 적절한 크기의 벡터 데이터를 얻을 수 있다. 이렇게 제한하지 않으면 훈련 데이터에 8만 8,585개의 고유한 단어가 포함된다. 이는 불필요하게 많으며, 많은 단어가 하나의 샘플에만 등장하기 때문에 분류 작업에 의미 있게 사용할 수 없다. 

<br>

변수 train_data와 test_data는 리뷰를 담은 배열이다. 각 리뷰는 단어 인덱스의 리스트이다. *단어 시퀀스가 인코딩 된 것* train_lebels와 test_lebbels는 부정을 나타내는 0과 긍정을 나타내는 1의 리스트이다. 

우선 train_data를 확인해보자. 
```py
train_data[0]
```
수행 결과는 다음과 같다. 

![4-1](assets/img/Deep_Learning/4-1jpg)

<br>

결과의 사진을 보면 한 눈에 알아보기 힘들게 되어 있다. 원인은 tensorflow의 문법일 수도 있고, colab 내부의 처리 문제일 수도 있다. 이를 해결하기 위해서는 한 번 `[]`로 묶어준다. 

![4-2](assets/img/Deep_Learning/4-2_수정jpg)

<br>

`[]`로 묶으면 위의 결과 사진과 같이 가로로 array 형태로 나타난다. 그리고 이제 train_data[0]으로 코드를 작성했을 때 세로로 불편하게 뜨는지 알 수 있다. train_data[0]은 list이지만, 데이터 타입이 객체이기 때문이다. 

<br>

다음으로 train_labels를 확인해보자.

```py
train_labels[0]
```

<br>

결과는 아래 사진과 같다. 그렇다면 train_lables[0]이 왜 '1'인가?에 대해서 위의 과정을 따라 다시 생각해보자. 대답하지 못하면 마우스로 스스로의 이마를 내려치면 된다. 

![4-3](assets/img/Deep_Learning/4-3jpg)

<br>

그렇다면 가장 큰 값은 얼마인가? 그리고 이 큰 값은 얼마인가? 왜 그런가?에 대해서 생각해보자. 

```py
max([max(sequence) for sequence in train_data])
```

<br>

가장 큰 값을 찾기 위해서는 위와 같은 코드 한 줄을 작성하면 된다. 
그렇게 하면 결과 값으로 '9999'가 나오는데, 이는 가장 자주 등장하는 단어 1만 개로 제한했기 때문에 단어 인덱스는 9,999를 넘지 않는다. 

<br>

## 4.1.2 데이터 준비하기 
신경망에 숫자 리스트를 바로 주입할 수는 없다. 이 숫자 리스트는 모두 길이가 다르지만 신경망은 동일한 크기의 배치를 원하기 때문이다. 여기서 list를 tensor로 바꾸는 방법은 2가지이다. 

1. 같은 길이가 되도록 list에 패팅(padding)을 추가하고 (samples, max_length) 크기의 정수 tensor로 변환한다. 그 다음, 이 정수 tensor를 다룰 수 있는 층으로 신경망을 시작한다. (이는 Embedding 층을 말하며, 이후에 자세히 다루도록 한다.)
2. list를 multi-hot encoding하여 0과 1의 벡터로 변환합니다. 예를 들어, 시퀀스 [8, 5]를 인덱스 8과 5의 위치는 1이고 그 외는 모두 0인 10,000차원 벡터로 각각 변환하는 것이다. 그 다음 부동 소수점 벡터 데이터를 다룰 수 있는 Dense층을 신경망의 첫 번째 층으로 사용한다. 

해당 예제에서는 2번쨰 방식을 사용하고, 이해를 돕기 위해 직접 데이터를 multi-hot Vector로 만들겠다. 

<br>

```py
import numpy as np

def vectorize_sequences(sequences, dimension=10000):
    # 크기가 (len(sequences), dimension)이고 모든 원소가 0인 행렬을 만든다. 
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        for j in sequence:
            # results[i]에 특정 인덱스의 위치를 1로 만든다. 
            results[i, j] = 1.
        return results

# 훈련 데이터를 벡터로 변환한다. 
x_train = vectorize_sequences(train_data)
# 테스트 데이터를 벡터로 변환한다. 
x_test = vectorize_sequences(test_data)
```

<br>

이제 샘플은 다음과 같이 나타나게 된다. 

![4-5](assets/img/Deep_Learning/4-5jpg)

<br>

그렇다면 이제 레이블도 쉽게 벡터로 바꿀 수 있다. 

```py
y_train = np.asarray(train_labels).astype("float32")
y_test = np.asarray(test_labels).astype("float32")
```

<br>

# `리뷰를 다시 텍스트로 디코딩 해보자!`
```py
word_index = imdb.get_word_index()
reverse_word_index = dict(
    [(value, key) for (key, value) in word_index.items()])
decoded_review = " ".join(
    [reverse_word_index.get(i - 3, "?") for i in train_data[0]])
```

<br>

여기까지, 이제 신경망에 주입할 데이터가 준비되었다. 이제 신경망 모델을 만들어보자. 

<br>

## 4.1.3 신경망 모델 만들기

입력 데이터는 벡터고 레이블은 스칼라(1 또는 0)이다. *아마 앞으로 볼 수 있는 문제 중에서 가장 간단할 것이다.* 이런 문제에 잘 작동하는 모델은 **relu 활성화 함수**를 사용한 밀집 연결 층을 그냥 쌓는 것이다. 

Dense층을 쌓을 때 두 가지 중요한 구조상의 결정이 필요하다. 
1. 얼마나 많은 층을 사용할 것인가?
2. 각 층에 얼마나 많은 유닛을 둘 것인가? 

<br>

해당 챕터에서는 이런 이론에 대한 원리에 대해 설명하지 않는다. Dense층을 쌓을 때 도움이 되는 일반적인 원리는 5장에서 다루므로, 잠시 4장을 진행하는 동안에는 책을 따라 아래 구조를 따라서 진행한다.

1. 16개의 유닛을 가진 2개의 중간층
2. 재 리뷰의 감정을 스칼라 값의 예측으로 출력하는 세 번째 층 

<br>

```py
# 모델 정의하기 
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
```

<br>

Dense층에 전달한 첫 번째 매개변수는 층의 유닛(unit) 개수이며, 층이 가진 표현 공간(representation space)의 차원이다. 2장과 3장에서 relu 활성화 함수를 활용한 Dense층은 다음과 같은 tensor 연산을 연결하여 구현 한다는 것을 배웠다. 

![relu_img](https://ko.d2l.ai/_images/chapter_deep-learning-basics_mlp_3_0.svg)

relu는 음수를 0으로 만드는 함수이다. 위의 relu 함수 그래프를 보면, 0이하로 내려가지 않기 때문에 음수가 될 수 없고 음수를 0으로 만든다는 것을 알 수 있다. 

<br>

![sigmoid_img](https://blog.kakaocdn.net/dn/bDtPLB/btqUBwxTfak/joJdoKgBAwaMZtBrRwmpK0/img.png)

Sigmoid는 임의의 값을 [0, 1] 사이로 압축하므로, 출력 값을 확률처럼 해석할 수 있다. 

<br>

이러한 이유로 중간층은 활성화 함수로 relu를 사용하고, 마지막 층은 "확률 0과 1사이의 점수로, 어떤 샘플이 타깃 '1'일 가승성이 높다는 것은 그 리뷱 긍정일 가능성이 높다는 것을 의미한다."를 출력하기 위해 sigmoid 활성화 함수를 사용한다. 

<br>

---
### `그렇다면 활성화 함수는 무엇인가요? 왜 필요한가요?`

relu와 같은 활성화 함수(또는 비선형성(non-linearity)라고도 부른다.)가 없다면, Dense층은 선형적인 연산인 점곱과 덧셈 2개로 구성된다. 

```
output = dot(W, input) + b
```
그러므로 이 층은 입력에 대한 선형 변환(아핀 변환)만 학습할 수 있다. 이 층의 가설 공간은 입력 데이터를 16차원의 공간으로 바꾸는 가능한 모든 ㅓㄴ형 변환의 집합이다. 이런 가설 공간은 매우 제약이 많으며, 선형 층을 깊게 쌓아도 여전히 하나의 선형연산이기 떄문에 여러 개로 구성하는 장점이 없다. 2장에서 모았듯이, 층을 추가하더라도 가설 공간이 확장되지는 않는다. 

가설 공간을 풍부하게 만들어 층을 깊게 만드는 장점을 살리기 위해서는 비선형성 또는 활성화 함수를 추가해야 한다. 

---

<br>

모델을 완성하기 위해서는 손실 함수와 옵티 마이저를 선택해야 한다. 이진 분류 문제이며, 모델의 출력이 확률이기 때문에, binary_crossentropy 손실이 적합하다. 이 함수가 유일한 선택지는 아니다. mean_squared_error도 사용할 수 있다. 그러나 binary_crossentropy 함수를 사용하는 이유는 확률을 출력하는 모델을 사용할 때는 크로스엔트로피가 최선의 선택이기 때문이다. 

> 크로스엔트로피(crossentropy)는 '정보 이론' 분야에서 온 개념으로, 확률 분포 간의 차이를 측정한다.해당 예제에서는 원본 분포와 예측 분포 사이를 측정하는데 사용한다. 

<br>

옵티마이저는 rmsprop를 사용한다. 이 옵티마이저는 일반적으로 거의 모든 문제에서 기본 선택으로 좋다고 한다. 그렇다면, 이제 모델을 컴파일 해보자.

```py
# 모델 컴파일하기
model.compile(optimizer="rmsprop",
            loss="binary_crossentropy",
            metrics=["accuracy"])
```

<br>

## 4.1.4 훈련 검증

3장에서 배웠듯, 딥러닝 모델은 훈련 데이터에서 평가해서는 절대 안 된다. 검증 세트를 사용하여 훈련 과정 중에 모델의 정확도를 모니터링하는 것이 표준 관행이며, 해당 예제에서는 다음과 같이 원본 훈련 데이터에서 1만 개의 샘플을 뗴어 검증 세트를 만들어 진행하겠다. 

```py
# 검증 세트 준비하기 
x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]
```

<br>

이제 512개의 sample씩 mini-batch를 만들어 20번의 epochs 동안 모델을 훈련시킨다. 훈련 데이터에 있는 모든 샘플에 대해 20번 반복한다. 동시에 따로 떼어 놓은 1만 개의 샘플에서 손실과 정확도를 측정할 것이다. 이렇게 하기 위해서는 validation_data 매개변수에 검증 데이터를 전달해야 한다. 

```py
# 모델 훈련하기
history = model.fit(partial_x_train, partial_y_train,
                    epochs = 20,
                    batch_size = 512,
                    validation_data=(x_val, y_val)
```

<br>

위의 과정을 잘 거쳐왔다면, 모델이 훈련이 되고 아래와 같은 결과가 나올 것이다. 

![4-6](assets/img/Deep_Learning/4-6jpg)

*CPU를 사용하도 epoch마다 2초가 걸리지 않는다. 전체 훈련 시간은 20초 이상 걸린다. epoch이 끝날 때마다 1만 개의 검증 샘플 데이터에서 손실과 정확도를 계산하기 때문에 약간씩 지연이 된다.*

<br>

이제 3장의 내용을 떠올려보자. 3장에서는 model.fit() 메소드에 History 객체를 반환한다고 배웠다. 이 객체는 훈련하는 동안 발생한 모든 정보를 담고 있는 dictionary인 history 속성을 가지고 있다. 한번 확인해 보자.

![4-7](assets/img/Deep_Learning/4-7jpg)

<br>

이 dictionary는 훈련과 검증하는 동안 모니터링할 측정 지표당 하나씩 모두 4개의 항목을 담고 있다. 이어지는 두 코드에서는 matplotlib을 사용하여 훈련과 검증 데이터에 대한 손실과 정확도를 그려보자. *모델의 무작위한 초기화 때문에 다른 사람과 결과가 조금 다를 수 있음을 참고하자.*

```py
# 훈련과 검증 손실 그리기
import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict["loss"]
val_loss_values = history_dict["val_loss"]
epochs = range(1, len(loss_values)+ 1)

plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show
```

![4-8](assets/img/Deep_Learning/4-8jpg)

```py
# 훈련과 검증 정확도 그리기
plt.clf() 
acc = history_dict["accuracy"]
val_acc = history_dict["val_accuracy"]

plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
```

![4-9](assets/img/Deep_Learning/4-91jpg)

>여기서, gnsfus thstlfdl epoch마다 감소하고, 훈련 정확도는 증가해야 하지만, 

> 결과는 모든 loss 값들이 같았다. 책을 따라했음에도 불구하고. 

<br>

원래라면, 경사 하강법 최적화를 사용할 때 반복마다 최소화되는 것이 손실이다.

원하는 결과가 아니므로, 다시 새로운 신경망을 만들고 다시 훈련해서 데이터를 평가해보자. 

```py
# 모델을 처음부터 다시 훈련하기 
model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss = "binary_crossentropy",
              metrics = ["accuracy"])

model.fit(x_train, y_train, epochs=20, batch_size=512)
result = model.evaluate(x_test, y_test)
result
```

```
[23]
13초
model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss = "binary_crossentropy",
              metrics = ["accuracy"])

model.fit(x_train, y_train, epochs=4, batch_size=512)

Epoch 1/4
49/49 [==============================] - 2s 31ms/step - loss: 0.6932 - accuracy: 0.4997
Epoch 2/4
49/49 [==============================] - 1s 30ms/step - loss: 0.6932 - accuracy: 0.4972
Epoch 3/4
49/49 [==============================] - 2s 35ms/step - loss: 0.6932 - accuracy: 0.4978
Epoch 4/4
49/49 [==============================] - 2s 44ms/step - loss: 0.6932 - accuracy: 0.4970
782/782 [==============================] - 2s 2ms/step - loss: 0.6932 - accuracy: 0.5000
```

<br>

최종 결과는 아래와 같다. 책과의 차이가 많은 점이 있다.
```
[0.693148136138916, 0.5000399947166443]
```

첫 번째 숫자 0.69는 테스트 손실이고, 두 번째 숫자 0.50은 테스트 정확도이다. 

책을 따라 한다면, 아주 단순한 방식으로도 88%의 정화도를 달성해야한다.

`어디서 문제가 일어났는지, 답글을 달아주면 고쳐보겠습니다. 여러 의견 부탁드립니다.`

<br>

## 4.1.5 훈련된 모델로 새로운 데이터에 대해 예측하기 

우선 계속해서 진행해보도록 하겠다. 모델을 훈련시킨 후 이를 실전하고 싶을 것이다. 

3장에서 배웠듯, predict 메소드를 사용해서 어떤 리뷰가 긍정일 확률을 예측할 수 있다. 

```py
model.predict(x_test)
```

```
782/782 [==============================] - 2s 2ms/step
array([[0.5241254 ],
       [0.50044835],
       [0.50044835],
       ...,
       [0.50044835],
       [0.50044835],
       [0.50044835]], dtype=float32)
```

이 모델은 현재 50% 확률로 돌아가고 있기 때문에, 정확한 확신이 어렵다. 

<br>

## 정리 
이번 예제를 통해 배운 것을 정리해보자. 

1. 원본 데이터를 신경망에 tensor로 주입하기 위해서는 꽤 많은 전처리가 필요하다. 단어 시퀀스는 이진 벡터로 인코딩 될 수 있지만, 다른 인코딩 방식도 있다. 
2. relu 활성화 함수와 함께 Dense 층을 쌓은 모델은 감성 분류를 포함하여 여러 종류의 문제에 적용할 수 있다. 따라서 앞으로 자주 사용하게 될지도 모른다. 
3. 출력 클래스가 2개인 이진 분류 문제에서 하나의 unit과 sigmoid 활성화 함수를 가진 Dense층으로 끝나야 한다. 이 모델의 출력은 확률을 나타내는 0과 1사이의 스칼라 값이다. 
4. 이진 분류 문제에서 이런 스칼라 시그모이드 출력에 대해 사용할 손실 함수는 binary_crossentropy이다.
5. rmsprop 옵티마이저는 문제에 상관없이 일반적으로 사용하기 좋은 선택이다.
6. 훈련 데이터에 대해 성능이 향상됨에 따라 신경망은 과대적합되기 시작하고 이전에 몬 적 없는 데이터에서는 결과가 점점 나빠지게 된다. 항상 훈련 세트 이외의 데이터에서 성능을 모델링 해야한다. 
---

<p>

# 4.12 뉴스 기사 분류 : 다중 분류 문제 

<br>

이전 예제에서는 밀집 연결 신경망을 사용하여 벡터 입력을 어떻게 2개의 클래스로 분류하는지 보았다. 그렇다면 2개 이상의 클래스가 있을 때는 어떻게 해야할까?

<br>

이번 예제에서는 Reuter 뉴스를 46개의 상호 배타적인 토픽으로 분류하는 신경망을 만들어 볼 예정이다. 클래스가 많기 떄문에, 이 문제는 **다중 분류(multiclass classification)**의 예이다. 각 데이터 포인트가 정확히 하나의 범주로 분류되기 때문에 좀 더 정확하게 말하자면 **단일 레이블 다중 분류(single-label, multiclass classification)** 문제이다. 

---
<br>

## 4.2.1 로이터 데이터셋

1986년 로이터에서 공개한 짧은 뉴스 기사와 토픽의 집합인 로이터 데이터셋을 사용하겠다. 

이 데이터셋은 텍스트 분류를 위해 널리 사용되는 간단한 데이터셋이다. 46개의 토픽이 있으며, 어떤 토픽은 다른 것에 비해 데이터가 많다. 각 토픽은 훈련 세트에 최소한 10개의 샘플을 가지고 있다. IMDB, MNIST와 마찬가지로 로이터 데이터셋은 케라스에 포함되어 있다. 

```PY
# 로이터 데이터셋 로드하기 
from tensorflow.keras.datasets import reuters

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)
```

<br>

앞의 예제와 동일하게 IMDB 데이터셋처럼 num_words=10000 매개변수는 데이터에서 가장 자주 등장하는 단어 1만 개로 제한한다. 

여기에는 8,982개의 훈련 샘플과 2,246개의 테스트 샘플이 있다. 

```
>>> len(train_data)
8982
>>> len(test_data)
2246
```

<br>


IMDB 리뷰처럼 각 샘플은 정수 리스트이다. (단어 인덱스)
```py
train_data[10]
```

```
array([list([1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979, 3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12])],
      dtype=object)
```

<br>

# `단어를 어떻게 디코딩 하나요?`

```py
word_index = reuters.get_word_index()
reverse_word_index = dict(
    [(value, key) for (key, value) in word_index.items()])

decoded_newswire = " ".join(
    [reverse_word_index.get(i-3, "?") for i in train_data[0]])
```

```py
train_labels[10]
```

<br>

다음과 같은 코드를 짜고 train_labels의 10째를 확인해보면, 아래와 같은 결과가 나온다.

샘플에 연결된 레이블은 토픽의 인덱스로 0과 45 사이의 정수이다. 
```
3
```

<br>

## 4.2.2 데이터 준비 

이전의 예제와 동일한 코드를 사용하여 데이터를 벡터로 변환해보자. 

```py
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
```

<br>

레이블을 벡터로 바꾸는 방법은 두 가지이다.
1. 레이블의 리스트를 정수 tensor로 변환하는 것
2. One-Hot Encoding을 사용하는 것. *범주형 데이터에 널리 사용되기 때문에, 범주형 인코딩(categorical encoding)이라고도 부른다. 해당 경우 레이블의 one-hot encoding은 각 레으블의 인덱스 자리는 1이고 나머지는 모두 0인 벡터이다.*

<br>

예를 들어, one-hot encoding을 사용한 방법은 아래와 같다. 

```py
def to_one_hot(labels, dimension=46):
  results = np.zeros((len(labels), dimension))
  for i, label in enumerate(labels):
    results[i, label] = 1.
  return results

y_train = to_one_hot(train_labels)
y_test = to_one_hot(test_labels)
```

<br>

케라스에서는 이를 위한 내장 함수가 있다. 

```py
from keras.utils.np_utils import to_categorical

y_train = to_categorical(train_labels)
y_test = to_categorical(test_labels)
```

<br>

## 4.2.3 모델 구성하기

해당 토픽 분류 문제는 이전의 영화 리뷰 문제와 비슷해 보일 것이다. 두 경우 모두 짧은 텍스트를 분류하는 것이다. 여기에서는 새로운 제약 사항이 추가되었다. 출력 클래스의 개수가 2개에서 46개로 늘어난 점이다. 출력 공간의 차원이 훨씬 커진 것이다. 

이전에 사용했던 것처럼 Dense층을 쌓으면 각 층은 이전 층의 출력에서 제공한 정보만 사용할 수 있다. 한 층이 분류 문제에필요한 일부 정보를 누락하면, 그 다음 층에서 이를 복원할 방법은 없다. 각 층은 잠재적으로 정보의 병목(information bottleneck)이 될 수 있습니다. 이전 예제에서 16차원을 가진 중간층을 사용했지만, 16차원 공간은 46개의 클래스로 구분하기에 너무 많은 제약이 있다.이렇게 규모가 작은 층은 유용한 정보를 완전히 잃게 되는 정보의 병목 지점처럼 동작할 수 있다. 

이러한 이유로 좀 더 큰 층을 사용해보겠다. 

```py
# 모델 정의하기
model = keras.Sequential([
    layers.Dense(64, activation="relu"),
    layers.Dense(64, activation="relu"),
    layers.Dense(64, activation="softmax")
])
```

<br>

이 구조에서 주목해야할 점이 2가지 있다. 

첫째로, 마지막 Dense 층의 크기가 46이다. 각 입력 샘플에 대해 46차원의 벡터를 출력한다는 뜻이다. 이 벡터의 각 원소(각 차원)는 각기 다른 출력 클래스가 인코딩된 것이다. 

둘째로, 마지막층에 softmax 활성화 함수를 사용된다는 것이다. 각 입력 샘플마다 46개의 출력 클래스에 대한 확률 분포를 출력한다. 즉, 46차원의 출력 벡터를 만들며 output[i]는 어떤 샘플이 클래스 i에 속할 확률이다. 46개의 값을 모두 더하면 1이 된다. 

<br>

이러한 문제에 사용할 최선의 손실 함수는 categorical_crossentropy이다. 이 함수는 두 확률 분포 사이의 거리를 측정한다. 여기에서는 모델이 출력한 확률 분포와 진짜 레이블의 분포 사이의 거리이다. 두 분포 사이의 거리를 최소화함으로써 진짜 레이블에 가능한 가까운 출력을 내도록 모델을 훈련하게 된다. 

```py
#  모델 컴파일 하기
model.compile(optimizer="rmsprop",
              loss="categorical_crossentropy",
              metrics=["accuracy"])
```

<br>

## 4.2.4 훈련 검증 

훈련 데이터에서 1,000개의 샘플을 따로 떼어서 검증 세트로 사용해보겠다. 

```py
# 검증 세트 준비하기
x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = y_train[:1000]
partial_y_train = y_train[1000:]
```

<br>

이제 20번의 epoch으로 모델을 훈련시킨다. 

```py
# 모델 훈련하기


```