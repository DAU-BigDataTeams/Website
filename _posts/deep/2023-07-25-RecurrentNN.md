---
title: ì¼€ë¼ìŠ¤ë¥¼ ì´ìš©í•´ ìˆœí™˜ ì‹ ê²½ë§(Recurrent Neural Network, RNN) ì´í•´í•˜ê¸°
layout: post   
categories : [ai]
image : /assets/img/study/deep/ch10/5.png
description: ë”¥ëŸ¬ë‹ 
customexcerpt: ì–´... ìˆœí™˜ ì‹ ê²½ë§ì— ëŒ€í•´ ì´í•´ë¥¼ í–ˆë‹¤? ê·¸ëŸ¼ í¬ìŠ¤íŠ¸ ë§ˆì§€ë§‰ì—ì„œ ì–¸ê¸‰í•˜ëŠ” ëŒ€íšŒì— ì°¸ì—¬í•´ë´ ìƒê¸ˆì´ 2ì²œë§Œì›ì´ì•¼ğŸ’°
---


<span class = "alert g">ì‘ì„±ì : ë°•ì •í˜„</span>


<!-- ì•„ë˜ 2ì¤„ì€ ëª©ì°¨ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•œ ì‹¬ë³¼ì´ë‹ˆ ê±´ë“¤ì§€ ë§ì•„ ì£¼ì„¸ìš” -->
* random line to make it work. This will be removed.
{:toc}



# ì‹œê³„ì—´ì„ ìœ„í•œ ë”¥ëŸ¬ë‹

**ì‹œê³„ì—´(timeseries)** ë°ì´í„°ëŠ” ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¸¡ì •í•˜ì—¬ ì–»ì€ ëª¨ë“  ë°ì´í„°ë¥¼ ë§í•œë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ ì£¼ì‹ì˜ ì¼ë³„ ê°€ê²©, ë„ì‹œì˜ ì‹œê°„ë³„ ì „ë ¥ ì†Œëª¨ëŸ‰, ìƒì ì˜ ì£¼ê°„ë³„ íŒë§¤ëŸ‰ ë“±ì´ í•´ë‹¹ëœë‹¤.

ì‹œê³„ì—´ ë°ì´í„°ëŠ” dynamicsë¥¼ ì´í•´í•´ì•¼ í•œë‹¤.  
> dynamics : ì£¼ê¸°ì„±, ì‹œê°„ì— ë”°ë¥¸ íŠ¸ëœë“œ, ê·œì¹™ì ì¸ í˜•íƒœ ë“±

ê°€ì¥ ì¼ë°˜ì ì¸ ì‹œê³„ì—´ ê´€ë ¨ ì‘ì—…ì€ **forecasting** ì˜ˆì¸¡ì´ë‹¤. **í˜„ì‹œì ì˜ ì‹œê³„ì—´ ë°ì´í„° ë‹¤ìŒì— ì¼ì–´ë‚  ê²ƒì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„.**

ì´ë²ˆ ì¥(10ì¥)ì€ ì˜ˆì¸¡ì— ëŒ€í•´ì„œ ì£¼ë¡œ ë‹¤ë£° ê²ƒì„.
ë‹¨, ì‹œê³„ì—´ë¡œ í•  ìˆ˜ ìˆëŠ” ì‘ì—…ì€ ì‹¤ì œë¡œ ë‹¤ì–‘í•˜ë‹¤.

- ë¶„ë¥˜ : í•˜ë‚˜ ì´ìƒì˜ ë²”ì£¼í˜• ë ˆì´ë¸”ì„ ì‹œê³„ì—´ì— ë¶€ì—¬
- ì´ë²¤íŠ¸ ê°ì§€ : ì—°ì†ëœ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì—ì„œ ì˜ˆìƒë˜ëŠ” íŠ¹ì • ì´ë²¤íŠ¸ ë°œìƒì„ ì‹ë³„.  
 (ì˜ˆë¥¼ ë“¤ì–´ í•˜ì´ ë¹…ìŠ¤ë¹„ ì´ëŸ°ê±° ê°ì§€)
- ì´ìƒì¹˜ íƒì§€(anomaly detection) : ì—°ì†ëœ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì—ì„œ ë°œìƒí•˜ëŠ” ë¹„ì •ìƒì ì¸ í˜„ìƒì„ ê°ì§€í•œë‹¤.  
(ì´ìƒì¹˜ ìƒ˜í”Œë¡œ í›ˆë ¨ì´ ë¶ˆê°€ëŠ¥ ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ ë¹„ì§€ë„ í•™ìŠµ)

ì‹œê³„ì—´ì„ ë‹¤ë£° ë•Œ ë§¤ìš° ë‹¤ì–‘í•œ ë¶„ì•¼ì— íŠ¹í™”ëœ ë°ì´í„° í‘œí˜„ ê¸°ë²•ì„ ë³¼ ìˆ˜ ìˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ **Fourier transform**ì— ëŒ€í•´ ë“¤ì–´ ë³´ì•˜ì„ ê²ƒì´ë‹¤.
> ì•„ë‹ˆìš”? ì²˜ìŒì¸ë°..

íŠ¹íˆ ì´ë²ˆ ì¥ì€ Recurrent Neural Network(RNN)ì— ëŒ€í•´ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì„ ë°°ìš´ë‹¤!

## ì˜¨ë„ ì˜ˆì¸¡ ë¬¸ì œ
----
ê±´ë¬¼ ì§€ë¶• ìœ„ì˜ ì„¼ì„œì—ì„œ ìµœê·¼ì— ê¸°ë¡í•œ ê¸°ì••, ìŠµë„ì™€ ê°™ì€ ë§¤ì‹œê°„ ì¸¡ì •ê°’ì˜ ì‹œê³„ì—´ì´ ì£¼ì–´ì¡Œì„ ë•Œ 24ì‹œê°„ ë’¤ ì˜¨ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤.

> ì¢€ ì–´ë µë‹¤

ì¼ë‹¨ ë°ì´í„°ë¥¼ ë‚´ë ¤ë°›ê³  ì••ì¶•ì„ í•´ì œí•˜ì.


```python
!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip
```

    --2023-07-25 00:03:43--  https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip
    Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.226.96, 54.231.160.24, 52.217.100.118, ...
    Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.226.96|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 13565642 (13M) [application/zip]
    Saving to: â€˜jena_climate_2009_2016.csv.zipâ€™
    
    jena_climate_2009_2 100%[===================>]  12.94M  18.8MB/s    in 0.7s    
    
    2023-07-25 00:03:44 (18.8 MB/s) - â€˜jena_climate_2009_2016.csv.zipâ€™ saved [13565642/13565642]
    
    


```python
!unzip jena_climate_2009_2016.csv.zip
```

    Archive:  jena_climate_2009_2016.csv.zip
      inflating: jena_climate_2009_2016.csv  
      inflating: __MACOSX/._jena_climate_2009_2016.csv  
    


```python
# ë‚ ì”¨ ë°ì´í„°ì…‹ ì¡°ì‚¬í•˜ê¸°

import os

fname = os.path.join("jena_climate_2009_2016.csv")

with open(fname) as f:
    data = f.read()

lines = data.split("\n")
header = lines[0].split(",")
lines = lines[1:]
print(header)
print(len(lines))
```

    ['"Date Time"', '"p (mbar)"', '"T (degC)"', '"Tpot (K)"', '"Tdew (degC)"', '"rh (%)"', '"VPmax (mbar)"', '"VPact (mbar)"', '"VPdef (mbar)"', '"sh (g/kg)"', '"H2OC (mmol/mol)"', '"rho (g/m**3)"', '"wv (m/s)"', '"max. wv (m/s)"', '"wd (deg)"']
    420451
    

ì´ ë°ì´í„°ì˜ í–‰ì€ 42ë§Œ 451 ì—´ì€ 14ê°œì´ë‹¤.

ì´ ë°ì´í„° ì „ì²´ë¥¼ ë„˜íŒŒì´ arrayë¡œ ë³€ê²½í•˜ì. ì˜¨ë„ë¥¼ í•˜ë‚˜ì˜ arrayë¡œ ë§Œë“¤ê³  ë‹¤ë¥¸ ë‚˜ë¨¸ì§€ ë°ì´í„°ë¥¼ ë˜ ë‹¤ë¥¸ arrayë¡œ ë§Œë“¤ì

> ìš°ë¦¬ì˜ ëª©ì ì€ ì˜¨ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹ˆ target ê°’ìœ¼ë¡œ ì˜¨ë„ë§Œ ë”°ë¡œ ê°€ì§€ê³  ìˆëŠ” ê²ƒ


```python
import numpy as np

temperature = np.zeros((len(lines),))
raw_data = np.zeros((len(lines), len(header)-1))

for i, line in enumerate(lines):
    values = [float(x) for x in line.split(",")[1:]] # Data Timeì€ ì œì™¸ë¨
    temperature[i] = values[1]
    raw_data[i,:] = values[:]

```


```python
import matplotlib.pyplot as plt

plt.plot(range(len(temperature)), temperature)
plt.title("Temperature over the entire period of the dataset")
plt.show()
```


    
![png](/assets/img/study/deep/ch10/output_7_0.png)
    


ì‹œê°„ì„ ì¢í˜€ì„œ ì²˜ìŒ 10ì¼ê°„ ì˜¨ë„ ë°ì´í„°ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„ë‹¤. 10ë¶„ë§ˆë‹¤ ê¸°ë¡ë˜ë¯€ë¡œ **í•˜ë£¨ì— ì´ 144ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ ìˆë‹¤.**

> 10ì¼ì´ë©´ 144 * 10 = 1440


```python
import matplotlib.pyplot as plt

plt.plot(range(len(temperature[:1440])), temperature[:1440])
plt.title("Temperature for the first 10 days of the dataset")
plt.show()
```


    
![png](/assets/img/study/deep/ch10/output_9_0.png)
    


ì´ ê·¸ë˜í”„ì—ì„œ ì¼ë³„ ì£¼ê¸°ì„±ì„ ë³¼ ìˆ˜ ìˆë‹¤. íŠ¹íˆ ë§ˆì§€ë§‰ 4ì¼ê°„ì„ ë³´ë©´ í™•ì‹¤í•©ë‹ˆë‹¤.

> ê·¸ë˜í”„ í•´ì„í•˜ëŠ” ë°©ë²•   
> Yì¶•ì€ ì˜¨ë„ë¥¼ ì˜ë¯¸, Xì¶•ì€ 10ë¶„ ë‹¨ìœ„ ì˜ë¦° êµ¬ê°„(144 = 1ì¼)

**í•­ìƒ ë°ì´í„°ì—ì„œ ì£¼ê¸°ì„±ì„ ì°¾ì•„ì•¼í•¨**
ì—¬ëŸ¬ ì‹œê°„ ë²”ìœ„ì— ê±¸ì¹œ ì£¼ê¸°ì„±ì€ ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì¤‘ìš”í•˜ê³  ë§¤ìš° ì¼ë°˜ì ì¸ ì„±ì§ˆì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê³„ì ˆë³„ ì „ë ¥ ì‚¬ìš©ëŸ‰ ê°™ì€?  

**ë•Œë¬¸ì— ì‹œê³„ì—´ ë°ì´í„°ë¥¼ íƒìƒ‰í•  ë•ŒëŠ” í•­ìƒ ì´ëŸ¬í•œ íŒ¨í„´ì„ ë¨¼ì € ì°¾ì•„ì•¼í•¨!!**


ì•ìœ¼ë¡œ ëª¨ë“  ì˜ˆì œëŠ” í›ˆë ¨ : ê²€ì¦ : í…ŒìŠ¤íŠ¸ = 50 : 25 : 25 ë¡œ ì§€ì •í•˜ê² ë‹¤.

**ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ ê²€ì¦ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ í›ˆë ¨ ë°ì´í„°ë³´ë‹¤ ìµœì‹ ì´ì–´ì•¼ í•œë‹¤.**

> ì™œ ê·¸ëŸ´ê¹Œ? ìŠ¤ìŠ¤ë¡œ ìƒê°í•´ë³´ê¸° ë°”ë€ë‹¤.



```python
# ê° ë¶„í• ì— ì‚¬ìš©í•  ìƒ˜í”Œ ê°œìˆ˜ ê³„ì‚°
num_train_samples = int(0.5 * len(raw_data))
num_val_samples = int(0.25 * len(raw_data))
num_test_samples = len(raw_data) - num_train_samples - num_val_samples

print(num_train_samples)
print(num_val_samples)
print(num_test_samples)
```

    210225
    105112
    105114
    

### ë°ì´í„° ì¤€ë¹„
----
ì´ ë¬¸ì œì˜ ì •í™•í•œ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

**í•œ ì‹œê°„ì— í•œ ë²ˆì”© ìƒ˜í”Œë§ëœ 5ì¼ê°„ì˜ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ 24ì‹œê°„ ë’¤ì˜ ì˜¨ë„ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆì„ê¹Œ?**

ë¨¼ì € ë°ì´í„°ë¥¼ ì‹ ê²½ë§ì— ì ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì „ì²˜ë¦¬ê°€ í•„ìš”í•˜ë‹¤.

ë°ì´í„°ê°€ ìˆ˜ì¹˜í˜•ì´ë‹ˆ ë²¡í„°í™”ë„ í•„ìš”ì—†ë‹¤. ë‹¨, ê° featureì˜ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— normalization ì´ í•„ìš”í•˜ë‹¤.


```python
# ë°ì´í„° ì •ê·œí™”

mean = raw_data[:num_train_samples].mean(axis=0)
raw_data -= mean


std = raw_data[:num_train_samples].std(axis=0)
raw_data /= std

```

ì´ì œ ê³¼ê±° 5ì¼ì¹˜ ë°ì´í„°ì™€ 24ì‹œê°„ ë’¤ íƒ€ê¹ƒ ì˜¨ë„ì˜ ë°°ì¹˜ë¥¼ ë°˜í™˜í•˜ëŠ” Dataset ê°ì²´ë¥¼ ë§Œë“¤ì–´ì•¼í•œë‹¤.

í˜„ì¬ ë°ì´í„°ëŠ” ì¤‘ë³µì´ ë§ë‹¤. ì¦‰, ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ê²½ìš° ë©”ëª¨ë¦¬ ë‚­ë¹„ê°€ ì‹¬ê°í•˜ë‹¤.  
ë”°ë¼ì„œ raw_dataì™€ temperature ë°°ì—´ë§Œ ë©”ëª¨ë¦¬ì— ìœ ì§€í•˜ê³  ê·¸ë•Œê·¸ë•Œ ìƒ˜í”Œì„ ìƒì„±í•˜ê² ë‹¤.

> keras ë‚´ì¥ ê¸°ëŠ¥ì¸ ```timeseries_dataset_from_array()```ë¥¼ ì‚¬ìš©í•´ì„œ ìƒ˜í”Œì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

í•´ë‹¹ ê¸°ëŠ¥ì˜ ì‚¬ìš© ë°©ë²•ì„ ì˜ˆì‹œë¥¼ í†µí•´ ë³´ì´ê² ìŒ

ìš°ì„ , ì‹œê³„ì—´ ë°ì´í„° ë°°ì—´ì„ ë§¤ê°œë³€ìˆ˜ë¡œ ì œê³µí•˜ë©´ ê²°ê³¼ë¡œ ì›ë³¸ ì‹œê³„ì—´ì—ì„œ ì¶”ì¶œí•œ ìœˆë„ìš°ë¥¼ ì œê³µí•œë‹¤.(ìš°ë¦° ì´ê±¸ sequenceë¼ê³  ë¶€ë¥¸ë‹¤.)

> ë¹…ë°ë¶„ ìˆ˜ì—… ë“¤ì—ˆìœ¼ë©´ ë­”ë§ì¸ì§€ ì•Œê±¸? ë“¤ì—ˆì–´ë„ ëª¨ë¥¸ë‹¤ë©´ ë§¤ìš° ë§¤ìš° ë§¤ìš° ë§¤ìš° ìœ ê°  
> ëª» ë“¤ì—ˆë‹¤ë©´ ìœ ê°






```python
import numpy as np
from tensorflow import keras

int_sequence = np.arange(10) # 0 ~ 9ê¹Œì§€ ì •ìˆ˜ ë°°ì—´ ìƒì„±
dummy_dataset = keras.utils.timeseries_dataset_from_array(
    data = int_sequence[:-3], # 0 1 2 3 4 5 6 ì„ ìƒ˜í”Œë§
    targets = int_sequence[3:], # ìœˆë„ìš° ë‹¤ìŒ ì˜ˆì¸¡ ê°’ì€ N+3
    sequence_length = 3, # ì‹œí€€ìŠ¤ì˜ ê¸¸ì´
    batch_size = 2 # ì‹œí€€ìŠ¤ì˜ ë°°ì¹˜ í¬ê¸°
)


for inputs, target in dummy_dataset:
    for i in range(inputs.shape[0]):
        print(f"ìœˆë„ìš° : {[int(x) for x in inputs[i]]}, ì˜ˆì¸¡ : {int(target[i])} ")
```

    ìœˆë„ìš° : [0, 1, 2], ì˜ˆì¸¡ : 3 
    ìœˆë„ìš° : [1, 2, 3], ì˜ˆì¸¡ : 4 
    ìœˆë„ìš° : [2, 3, 4], ì˜ˆì¸¡ : 5 
    ìœˆë„ìš° : [3, 4, 5], ì˜ˆì¸¡ : 6 
    ìœˆë„ìš° : [4, 5, 6], ì˜ˆì¸¡ : 7 
    

ëŒ€~ì¶© ë­”ì§€ ì•Œê² ì œ?

ê·¸ëŸ¼ ìš°ë¦¬ ì˜ˆì œì— ì ìš©í•´ë³´ê²Œì“°

- sampling_rate = 6, ì‹œê°„ë‹¹ í•˜ë‚˜ì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ ìƒ˜í”Œë§ë¨. ì¦‰, 6ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©
- sequence_length = 120, ì´ì „ 5ì¼ê°„(120ì‹œê°„) ë°ì´í„° ì‚¬ìš©
- delay = sampling_rate * (sequence_length + 24 - 1), ì‹œí€€ìŠ¤ì˜ íƒ€ê¹ƒì€ ì‹œí€€ìŠ¤ ëì—ì„œ 24ì‹œê°„ í›„ ì˜¨ë„

í›ˆë ¨ ë°ì´í„°ì…‹ì„ ë§Œë“¤ ë•Œ ì²˜ìŒ 50%ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ `start_index`ì˜ ê°’ì„ 0, `end_index`ì˜ ê°’ì„ `num_train_samples`ë¡œ ì§€ì •  

ê²€ì¦ ë°ì´í„°ì…‹ì˜ ê²½ìš° ê·¸ë‹¤ìŒ 25% ì‚¬ìš©ì„ ìœ„í•´ `start_index=num_train_samples`ì™€ `end_index=num_train_samples + num_val_samples`ë¡œ ì§€ì •

í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì˜ ê²½ìš° ë‚¨ì€ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ `start_index=num_train_samples + num_val_samples`ë¡œ ì§€ì •í•œë‹¤.


```python
# í›ˆë ¨, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë§Œë“¤ê¸°

# ë§¤ê°œë³€ìˆ˜ ì„¤ì •
sampling_rate = 6
sequence_length = 120
delay = sampling_rate * (sequence_length + 24 - 1)
batch_size = 256

# í›ˆë ¨ ë°ì´í„°
train_dataset = keras.utils.timeseries_dataset_from_array(
    raw_data[:-delay],
    targets = temperature[delay:],
    sampling_rate = sampling_rate,
    sequence_length = sequence_length,
    shuffle = True,
    batch_size = batch_size,
    start_index = 0,
    end_index = num_train_samples
)

# ê²€ì¦ ë°ì´í„°
val_dataset = keras.utils.timeseries_dataset_from_array(
    raw_data[:-delay],
    targets = temperature[delay:],
    sampling_rate = sampling_rate,
    sequence_length = sequence_length,
    shuffle = True,
    batch_size = batch_size,
    start_index = num_train_samples,
    end_index = num_train_samples + num_val_samples
)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°
test_dataset = keras.utils.timeseries_dataset_from_array(
    raw_data[:-delay],
    targets = temperature[delay:],
    sampling_rate = sampling_rate,
    sequence_length = sequence_length,
    shuffle = True,
    batch_size = batch_size,
    start_index = num_train_samples + num_val_samples
)
```

ê° ë°ì´í„°ì…‹ì€ (samples, targets) í¬ê¸°ì˜ íŠœí”Œì„ ë°˜í™˜í•¨. samplesëŠ” 256ê°œì˜ ìƒ˜í”Œ ë°°ì¹˜ì„.

ê° ìƒ˜í”Œì€ ì—°ì†ëœ 120ì‹œê°„ì˜ ì…ë ¥ ë°ì´í„°ë¥¼ ë‹´ê³  ìˆë‹¤. targetì€ 256ê°œì˜ íƒ€ê¹ƒ ì˜¨ë„ì— í•´ë‹¹í•˜ëŠ” ë°°ì—´ì´ë‹¤.

ìƒ˜í”Œì´ ì„ì—¬ìˆìœ¼ë‹ˆ (shuffle = True) ë°°ì¹˜ì— ìˆëŠ” ì—°ì†ëœ ë‘ ìƒ˜í”Œì´ ê¼­ ì‹œê°„ì ìœ¼ë¡œ ê°€ê¹ë‹¤ ë³´ì¥ ëª» í•¨.


```python
# í›ˆë ¨ ë°ì´í„°ì…‹ì˜ ë°°ì¹˜ í¬ê¸° í™•ì¸
for samples, targets in train_dataset:
    print(f"ìƒ˜í”Œ í¬ê¸°:{samples.shape}")
    print(f"íƒ€ê¹ƒ í¬ê¸°:{targets.shape}")
    break
```

    ìƒ˜í”Œ í¬ê¸°:(256, 120, 14)
    íƒ€ê¹ƒ í¬ê¸°:(256,)
    

### ìƒì‹ ìˆ˜ì¤€ì˜ ê¸°ì¤€ì 
----

ì±…ì—ì„œ ë”¥ëŸ¬ë‹ ì‚¬ìš© ì „ ê°„ë‹¨í•œ ìƒì‹ ìˆ˜ì¦Œì˜ í•´ë²•ì„ ì‹œë„í•œë‹¤. ì •ìƒì ì¸ ë¬¸ì œì¸ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ìš©ë„ì´ë©° ê³ ìˆ˜ì¤€ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì´ë¼ë©´ ë›°ì–´ë„˜ì–´ì•¼ í•  ê¸°ì¤€ì ì´ë‹¤.

ì´ëŸ° ìƒì‹ ìˆ˜ì¤€ì˜ í•´ë²•ì€ ì•Œë ¤ì§„ í•´ê²°ì±…ì´ ì—†ëŠ” ìƒˆë¡œìš´ ë¬¸ì œë¥¼ ë‹¤ë£¨ì–´ì•¼ í•  ë•Œ ìœ ìš©í•˜ë‹¤.

> ë°ì´í„° ë¶ˆê· í˜• ê°™ì€ ë¬¸ì œ ì²´í¬?

ë‚ ì”¨ ì˜¨ë„ë¼ëŠ”ê²Œ ì–´ì œ ì˜¤ëŠ˜ í¬ê²Œ ì°¨ì´ë‚˜ëŠ” ê·¸ëŸ°ê±´ ì•„ë‹ˆë‹¤. ê·¸ë˜ì„œ ì§€ê¸ˆìœ¼ë¡œë¶€í„° 24ì‹œê°„ ë’¤ ì˜¨ë„ëŠ” ì§€ê¸ˆê³¼ ë™ì¼í•˜ë‹¤ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤.


```python
# ìƒì‹ ìˆ˜ì¤€ ëª¨ë¸ì˜ MAE ê³„ì‚°í•˜ê¸°
def evaluate_naive_method(dataset):
    total_abs_err = 0
    samples_seen = 0

    for samples, targets in dataset:
        preds = samples[:, -1, 1] * std[1] + mean[1] # ì˜¨ë„ íŠ¹ì„±ì€ ì»¬ëŸ¼ ì¸ë±ìŠ¤ 1ì— ìˆë‹¤. ë”°ë¼ì„œ samples[:, -1, 1]ì´ ì…ë ¥ ì‹œí€€ìŠ¤ ë§ˆì§€ë§‰ ì˜¨ë„ ê°’ì„.
        # ì •ê·œí™” í–ˆìœ¼ë‹ˆ ì›ë˜ ê°’ìœ¼ë¡œ ë³€ê²½í•˜ê¸° ìœ„í•´ ì •ê·œí™” ë³µêµ¬
        total_abs_err += np.sum(np.abs(preds - targets))
        samples_seen += samples.shape[0]

    return total_abs_err / samples_seen


print(f"ê²€ì¦ MAE: {evaluate_naive_method(val_dataset):.2f}")
print(f"í…ŒìŠ¤íŠ¸ MAE: {evaluate_naive_method(test_dataset):.2f}")
```

    ê²€ì¦ MAE: 2.44
    í…ŒìŠ¤íŠ¸ MAE: 2.62
    

MAE ê²°ê³¼ëŠ” ê° 2.44, 2.62ê°€ ë‚˜ì˜¨ ëª¨ìŠµì´ë‹¤. ë”°ë¼ì„œ 24ì‹œê°„ ë’¤ ì˜¨ë„ë¥¼ ì˜ˆì¸¡í•˜ë©´ í‰ê· ì ìœ¼ë¡œ 2.5ë„ ì°¨ì´ê°€ ë‚  ê²ƒì´ë‹¤.

> (2.44 + 2.62) / 2 = 2.53

ë‚˜ëŠ” ê°œì¸ì ìœ¼ë¡œ ë§Œì¡±í•¨. ê·¼ë° ì´ëŸ° ê·œì¹™ì„ ê¸°ë°˜ìœ¼ë¡œ ë‚ ì”¨ ì˜ˆë³´ë¥¼ í•˜ì§€ëŠ” ì•Šìœ¼ë‹ˆ ë”¥ëŸ¬ë‹ë„ í•´ë³´ì
> ì‚¬ì‹¤ ì´ê±° ê²°ê³¼ê°€ ì ˆëŒ€ ì¢‹ê²Œ ë‚˜ì˜¬ ìˆ˜ ì—†ì—ˆì„ ê²ƒ ê°™ìŒ ì¢‹ê²Œ ë‚˜ì™”ìœ¼ë©´ ì €ìëŠ” ì˜ˆì œ ë³€ê²½í•´ì•¼í•¨.   

> ìš°ë¦¬ë‚˜ë¼ ê¸°ìƒì²­ì€ ë­... ìƒëµ


### ê¸°ë³¸ì ì¸ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ ì‹œë„
----

RNNì²˜ëŸ¼ ë³µì¡í•˜ê³  ì—°ì‚° ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ëª¨ë¸ì„ ì‹œë„í•˜ê¸° ì „ì— ê°„ë‹¨í•˜ê³  ì†ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸(ex. ì†Œê·œëª¨ ì‹ ê²½ë§)ì„ ë¨¼ì € ë§Œë“œëŠ” ê²ƒì´ ì¢‹ë‹¤.

ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ë³µì¡í•œ ë°©ë²•ì„ ë„ì…í•˜ëŠ” ê·¼ê±°ê°€ ë§ˆë ¨ë˜ê³  ì‹¤ì œì ì¸ ì´ë“ë„ ì–»ì„ ê²ƒì„.


> ì—¬ë‹´ì´ì§€ë§Œ, í˜„ì¬ ì—°êµ¬ì†Œ ì¸í„´ì„ í•˜ë©´ì„œ ë°•ì‚¬ë‹˜ë“¤ì˜ ì‘ì—… í”„ë¡œì„¸ìŠ¤ë¥¼ ë³´ë©´ ì¼ë‹¨ ì‹¤í—˜ì„ ë§¤ìš° ë§¤ìš° ë§¤ìš° ë§ì´í•˜ê³  ë³´ì™„í•  ì ì„ ê¸°ë¡í•œë‹¤. ê·¸ë¦¬ê³  ë‹¤ìŒ ì‹¤í—˜ì—ì„œ ê°œì„ ì‹œí‚¨ë‹¤....




```python
# ë°€ì§‘ ì—°ê²° ëª¨ë¸ í›ˆë ¨í•˜ê³  í‰ê°€í•˜ê¸°

from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf

inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.Flatten()(inputs)
x = layers.Dense(16, activation = "relu")(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

with tf.device('/gpu:0'):
    # ìµœìƒì˜ ëª¨ë¸ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ì½œë°± ì •ì˜
    callbacks = [
        keras.callbacks.ModelCheckpoint("jena_dens.keras", save_best_only = True)
    ]

    model.compile(optimizer = "rmsprop",
                loss = "mse",
                metrics = ["mae"])

    history = model.fit(train_dataset,
                        epochs = 10,
                        validation_data = val_dataset,
                        callbacks = callbacks)

model = keras.models.load_model("/content/jena_dens.keras") # ìµœìƒì˜ ëª¨ë¸ ë‹¤ì‹œ ë¡œë“œí•˜ê³  ë°ì´í„°ì—ì„œ í‰ê°€

print(f"í…ŒìŠ¤íŠ¸ MAE:{model.evaluate(test_dataset)[1]:.2f}")
```


```python
keras.utils.plot_model(model, "model2.png")
```




    
![png](/assets/img/study/deep/ch10/output_25_0.png)
    




```python
import matplotlib.pyplot as plt

loss = history.history["mae"]
val_loss = history.history["val_mae"]
epochs = range(1, len(loss) + 1)

plt.figure()
plt.plot(epochs, loss, "bo", label = "Training MAE")
plt.plot(epochs, val_loss, "b", label = "Validation MAE")

plt.title("Training and validation MAE")
plt.legend()
plt.show()
```


    
![png](/assets/img/study/deep/ch10/output_26_0.png)
    


ì¼ë¶€ val_lossëŠ” í•™ìŠµì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê¸°ì¤€ì ì— ê°€ê¹ì§€ë§Œ ì•ˆì •ì ì´ë¼ê³ ëŠ” ë³¼ ìˆ˜ ì—†ë‹¤.
> Xì¶• ì—í¬í¬, Yì¶• loss  

ì•ì—ì„œ ìƒì‹ìœ¼ë¡œ êµ¬í–ˆë˜ MAE ê°’ê³¼ ë‹¤ì†Œ ì°¨ì´ê°€ ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. í•´ë‹¹ ëª¨ë¸ì´ ìƒì‹ì´ ì—†ë‹¤!

ê°„ë‹¨í•˜ê³  ê´œì°®ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ì´ ë°ì´í„°ì™€ íƒ€ê¹ƒì„ ë§¤í•‘í•  ìˆ˜ ìˆë‹¤ë©´ ì™œ í›ˆë ¨í•œ ëª¨ë¸ì€ ì´ë¥¼ ëª» ì°¾ê³  ì„±ëŠ¥ì´ ë‚®ì„ê¹Œ?

ì•„ë¬´ë¦¬ ë¨¸ì‹ ëŸ¬ë‹ì´ê³  ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©í•œë‹¤ì§€ë§Œ ìš°ë¦¬ê°€ ì„¤ì •í•œ ëª¨ë¸ì˜ 2ê°œ ì¸µì€ ê²°êµ­ ë„¤íŠ¸ì›Œí¬ì˜ ê°€ëŠ¥í•œ ëª¨ë“  ê°€ì¤‘ì¹˜ ì¡°í•©ì´ë‹¤.(ê°€ì„¤ê³µê°„)

ì¦‰, ê²½ì‚¬ í•˜ê°•ë²•ì´ í•„ìˆ˜ì ìœ¼ë¡œ ì¢‹ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ë‹¤ ì£¼ì§€ëŠ” ì•ŠëŠ”ë‹¤.


### 1D ConvNet ì‚¬ìš©
----

ì…ë ¥ ì‹œí€€ìŠ¤ëŠ” ì¼ë³„ ì£¼ê¸°ë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì— í•©ì„±ê³± ëª¨ë¸ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤.

ì‹œê°„ ì¶•ì— ëŒ€í•œ í•©ì„±ê³±ì€ ë‹¤ë¥¸ ë‚ ì— ìˆëŠ” ë™ì¼í•œ í‘œí˜„ì„ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

> ë‚ ì”¨ê°€ ë‹¤ìŒë‚  ì´ì‹œê°„ê³¼ ë§¤ìš° í° ì°¨ì´ëŠ” ì—†ê¸° ë•Œë¬¸

ì´ë¯¸ conv2Dì™€ SeparableConv2D ì¸µì— ëŒ€í•´ í•™ìŠµ í–ˆë‹¤.  
ì´ ì¸µë“¤ì€ ì‘ì€ ìœˆë„ìš°ë¡œ 2D ê·¸ë¦¬ë“œ ìœ„ë¥¼ ì´ë™í•˜ë©´ì„œ ì…ë ¥ì„ ë°”ë¼ë³¸ë‹¤. 1D & 3D í•©ì„±ê³±ë„ ì¡´ì¬í•œë‹¤.

- conv1DëŠ” 1D ìœˆë„ìš°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ìŠ¬ë¼ì´ë”©
- conv3D ì¸µì€ ì •ìœ¡ë©´ì²´ ìœˆë„ìš°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë³¼ë¥¨ ìœ„ë¥¼ ìŠ¬ë¼ì´ë”©

![3](https://pic2.zhimg.com/v2-5cba44a31321d5e26d5f96818156cbfd_r.jpg)

<div align=center>
conv3Dê°€ ìŠ¬ë¼ì´ë”© í•˜ëŠ” ëŠë‚Œì´ë¼ ìƒê°í•˜ì.
</div>


conv2Dì™€ ë§¤ìš° ìœ ì‚¬í•œ conv1Dë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤. **ì‹œí€€ìŠ¤ ìœ„ë¡œ ìœˆë„ìš°ë¥¼ ìŠ¬ë¼ì´ë”©í•˜ë©´ ìœˆë„ìš° ì•ˆì˜ ë‚´ìš©ì´ ìœ„ì¹˜ì— ìƒê´€ì—†ì´ ë™ì¼í•œ ì„±ì§ˆì„ ê°€ì§„ë‹¤ëŠ” ì˜ë¯¸ë‹¤.** (ì´ë¥¼ í‰í–‰ ì´ë™ ë¶ˆë³€ì„± ê°€ì •ì´ë¼í•¨.)

ì´ë¥¼ ì˜¨ë„ ì˜ˆì¸¡ ë¬¸ì œì— ì ìš©í•´ ë³´ê² ë‹¤. ì´ˆê¸° ìœˆë„ìš° ê¸¸ì´ëŠ” 24ë¡œ ì •í•œë‹¤. (í•œ ì£¼ê¸° 24ì‹œê°„ ë°ì´í„°)

MaxPooling1D ì¸µìœ¼ë¡œ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ìš´ìƒ˜í”Œë§í•˜ê¸° ë•Œë¬¸ì— ê·¸ì— ë§ì¶”ì–´ ìœˆë„ìš° í¬ê¸°ë¥¼ ì¤„ì¸ë‹¤.



```python
inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.Conv1D(8,24,activation = "relu")(inputs)
x = layers.MaxPooling1D(2)(x)

x = layers.Conv1D(8,12,activation = "relu")(x)
x = layers.MaxPooling1D(2)(x)

x = layers.Conv1D(8,6,activation = "relu")(x)
x = layers.GlobalAveragePooling1D()(x)

outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

with tf.device("/gpu:0"):
    callbacks = [
        keras.callbacks.ModelCheckpoint("jena_conv.keras",
                                        save_best_only = True)
    ]

    model.compile(optimizer = "rmsprop", loss = "mse", metrics = ["MAE"])

    history = model.fit(train_dataset,
                        epochs = 10,
                        validation_data = val_dataset,
                        callbacks = callbacks)

    model = keras.models.load_model("/content/jena_conv.keras")

    print(f"í…ŒìŠ¤íŠ¸ MAE: {model.evaluate(test_dataset)[1]:.2f}")
```


```python
keras.utils.plot_model(model, "model1.png")
```




    
![png](/assets/img/study/deep/ch10/output_30_0.png)
    




```python
import matplotlib.pyplot as plt

loss = history.history["MAE"]
val_loss = history.history["val_MAE"]
epochs = range(1, len(loss) + 1)

plt.figure()
plt.plot(epochs, loss, "bo", label = "Training MAE")
plt.plot(epochs, val_loss, "b", label = "Validation MAE")

plt.title("Training and validation MAE")
plt.legend()
plt.show()
```

Dense ë³´ë‹¤ ì„±ëŠ¥ì´ ë” ë‚˜ì˜ë‹¤;;

ë¬´ì—‡ì´ ë¬¸ì œì¼ì§€ ìƒê°ì„ í•´ë³´ì.

ìš°ì„  íŒíŠ¸ë¥¼ ì£¼ìë©´ ì•ì—ì„œ ë‚´ê°€ ë‚ ì”¨ëŠ” í‰í–‰ ì´ë™ ë¶ˆë³€ì„±ì„ ê°€ì •í•œë‹¤ê³  í–ˆë‹¤.

ë‹¤ë¥¸ ì´ìœ  í•˜ë‚˜ëŠ” í•´ë‹¹ ë°ì´í„°ì˜ ìˆœì„œê°€ ì¤‘ìš”í•˜ë‹¤ëŠ” ì .  
ìµœê·¼ ë°ì´í„°ê°€ 5ì¼ ì „ ë°ì´í„°ë³´ë‹¤ ë‚´ì¼ì˜ ì˜¨ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì— ë” ìœ ìš©í•˜ë‹¤. conv1DëŠ” ì´ëŸ° ì‚¬ì‹¤ì„ í™œìš©í•  ìˆ˜ ì—†ë‹¤.

> **ìµœëŒ€ í’€ë§, ì „ì—­ í‰ê·  í’€ë§ ì¸µ ë•Œë¬¸ì— ìˆœì„œ ì •ë³´ê°€ ë§ì´ ì‚¬ë¼ì§.**

> [ë¦¬ë§ˆì¸ë“œ]  
> MaxPooling : n strideë¡œ ì„ íƒëœ í•„í„°ì—ì„œ ê°€ì¥ í° ê°’ì„ ë½‘ëŠ”ë‹¤.  
> GlobalAveragePooling : layerì—ì„œ NxN  strideë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°’ì„ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, layer ì „ì²´ì—ì„œ í‰ê· ì„ ë½‘ì•„ë²„ë¦°ë‹¤.  
> ì‚¬ìš© ì´ìœ  => ê³¼ì í•© ë°©ì§€, í’€ë§ì„ í†µí•´ NxN í•„í„°ë“¤ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì§€ ì•Šê³  ê° í•„í„°ì—ì„œ íŠ¹ì • ê°’ 1ê°œë¥¼ ì¶”ì¶œí•´ì„œ ìƒˆë¡œìš´ í•„í„°ë¥¼ ìƒì„±í•¨.

![2](https://gaussian37.github.io/assets/img/dl/concept/gap/4.png)

<div align=center>
MaxPooling ì˜ˆì‹œ(ê·¸ë¦¼. gaussian37.github.io)
</div>

![3](https://gaussian37.github.io/assets/img/dl/concept/gap/5.png)

<div align=center>
Global AveragePooling ì˜ˆì‹œ (ê·¸ë¦¼. gaussian37.github.io)
</div>

> ë”°ë¼ì„œ ìµœëŒ€ í’€ë§, ì „ì—­ í‰ê·  í’€ë§ì„ ì´ìš©í•˜ë©´ ìˆœì„œ ì •ë³´ê°€ ë§ì´ ì‚¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.(ìˆœì„œë¥¼ ê³ ë ¤í•˜ê³  ë½‘ëŠ”ê²Œ ì•„ë‹Œ ìˆ«ìë“¤ì˜ íŠ¹ì§•ì„ ì´ìš©)



### ì²« ìˆœí™˜ ì‹ ê²½ë§(RNN no ì¼ë‹¨ LSTM)

Dense ëª¨ë¸ì€ ìš°ë¦¬ê°€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ```Flatten()```ìœ¼ë¡œ í¼ì³ë²„ë ¤ì„œ ì‹œê°„ ê°œë…ì„ ì—†ì•´ë‹¤.

í•©ì„±ê³± ëª¨ë¸ì˜ ê²½ìš° ëª¨ë“  ë¶€ë¶„ì„ ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í–ˆìœ¼ë©° í’€ë§ì„ ì ìš©í•˜ì—¬ ìˆœì„œ ì •ë³´ë¥¼ ìƒë„ë¡ í–ˆë‹¤.

ë”°ë¼ì„œ ì´ë²ˆì—” ì¸ê³¼ ê´€ê³„, ìˆœì„œê°€ ì˜ë¯¸ ìˆëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë³¸ë‹¤.

ì°¸ê³ ë¡œ ì§€ê¸ˆ ë³´ì¼ `LSTM(Long Short-Term Memory)`ëŠ” ì˜¤ë«ë™ì•ˆ ì¸ê¸°ê°€ ë§ì•˜ë‹¤.


```python
inputs = keras.Input(shape = (sequence_length, raw_data.shape[1]))
x = layers.LSTM(16)(inputs)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_lstm.keras",
                                    save_best_only = True)
]

model.compile(optimizer = "rmsprop",
              loss = "mse",
              metrics = ["mae"])

history = model.fit(train_dataset,
                    epochs = 2,
                    validation_data = val_dataset,
                    callbacks = callbacks)

model = keras.models.load_model("/content/jena_lstm.keras")

print(f"í…ŒìŠ¤íŠ¸ MAE: {model.evaluate(test_dataset)[1]:.2f}")
```

    Epoch 1/2
    819/819 [==============================] - 51s 58ms/step - loss: 40.4400 - mae: 4.6492 - val_loss: 12.6347 - val_mae: 2.7283
    Epoch 2/2
    819/819 [==============================] - 51s 62ms/step - loss: 11.4477 - mae: 2.6242 - val_loss: 10.2691 - val_mae: 2.4910
    405/405 [==============================] - 14s 34ms/step - loss: 14.1066 - mae: 2.7082
    í…ŒìŠ¤íŠ¸ MAE: 2.71
    


```python
keras.utils.plot_model(model, "model3.png")
```




    
![png](/assets/img/study/deep/ch10/output_35_0.png)
    




```python
import matplotlib.pyplot as plt

loss = history.history["mae"]
val_loss = history.history["val_mae"]
epochs = range(1, len(loss) + 1)

plt.figure()
plt.plot(epochs, loss, "bo", label = "Training MAE")
plt.plot(epochs, val_loss, "b", label = "Validation MAE")

plt.title("Training and validation MAE")
plt.legend()
plt.show()
```

í™•ì‹¤íˆ LSTM ëª¨ë¸ì´ ì‹œí€€ìŠ¤ êµ¬ì¡°ì™€ ì˜ ë§ì•„ì„œ ê·¸ëŸ°ê°€ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ìŠµì„ ë³´ì¸ë‹¤.

## ìˆœí™˜ ì‹ ê²½ë§ ì´í•´í•˜ê¸°
----

ë°€ì§‘ ì—°ê²° ë„¤íŠ¸ì›Œí¬ë‚˜ ì»¨ë¸Œë„·ì²˜ëŸ¼ ì§€ê¸ˆê¹Œì§€ ë³¸ ëª¨ë“  ì‹ ê²½ë§ì˜ íŠ¹ì§•ì€ ë©”ëª¨ë¦¬ê°€ ì—†ë‹¤!

ì…ë ¥ì€ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ë©° ì…ë ¥ê°„ì— ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì–´ìˆëŠ” ìƒíƒœëŠ” ì—†ë‹¤.

ì¦‰, ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì„œ ì‹œê³„ì—´ ì²˜ë¦¬ë¥¼ í•˜ë ¤ë©´ ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ì¤˜ì•¼í•œë‹¤.(ë“¤ê³ ìˆì§€ ì•Šìœ¼ë‹ˆ í•œë²ˆì— ì¤˜ì•¼ì§€)

ì´ëŸ° ë„¤íŠ¸ì›Œí¬ë¥¼ `í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬(feedforward network)`ë¼ê³  ë¶€ë¥¸ë‹¤.

ë°˜ëŒ€ë¡œ ì‚¬ëŒì€ ë¬¸ì¥ì„ ì½ì„ ë•Œ ì´ì „ì— ë‚˜ì˜¨ ê²ƒì„ ê¸°ì–µí•˜ë©´ì„œ ëˆˆìœ¼ë¡œ ì½ëŠ” ë§Œí¼ ë¨¸ë¦¬ì†ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.

ì¦‰, ì ì§„ì ìœ¼ë¡œ ì´ì „ ì •ë³´ì™€ ìƒˆë¡œìš´ ì •ë³´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬í•œë‹¤.

ìˆœí™˜ ì‹ ê²½ë§ì€ ì¸ê°„ì˜ ì´ëŸ° ëª¨ìŠµì„ ë¬˜ì‚¬í–ˆë‹¤. ê³¼ê±° ì •ë³´ë¥¼ ì‚¬ìš©í•´ì„œ êµ¬ì¶•ë˜ë©° ìƒˆë¡­ê²Œ ì–»ì€ ì •ë³´ë¥¼ ê³„ì† ì—…ë°ì´íŠ¸í•œë‹¤.  
ê·¹ë‹¨ì ìœ¼ë¡œ ë‹¨ìˆœí™” í–ˆì§€ë§Œ ìˆœí™˜ ì‹ ê²½ë§(RNN)ì€ ê°™ì€ ì›ë¦¬ë¥¼ ì ìš©í•œ ê²ƒì´ë‹¤.  
ì‹œí€€ìŠ¤ë¥¼ ìˆœíšŒí•˜ë©´ì„œ ì§€ê¸ˆê¹Œì§€ ì²˜ë¦¬í•œ ì •ë³´ë¥¼ **ìƒíƒœ**ì— ì €ì¥í•œë‹¤.

![5](/assets/img/study/deep/ch10/1.png)

RNNì˜ ìƒíƒœëŠ” 2ê°œì˜ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‚¬ì´ì— ì¬ì„¤ì •ëœë‹¤. ë”°ë¼ì„œ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¥¼ ì—¬ì „íˆ í•˜ë‚˜ì˜ ë°ì´í„° í¬ì¸íŠ¸ ì¦‰, ë„¤íŠ¸ì›Œí¬ì— ì…ë ¥ë˜ëŠ” ê°’ìœ¼ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆë‹¤.

> loop í˜•íƒœë¼ëŠ” ì†Œë¦¬ì„

ë£¨í”„ ìƒíƒœì— ëŒ€í•œ ê°œë…ì„ ëª…í™•íˆ í•˜ê¸° ìœ„í•´ ê°„ë‹¨í•œ RNN ì •ë°©í–¥ ê³„ì‚°ì„ êµ¬í˜„í•´ ë³´ì. ì´ RNNì€ (timesteps, input_features) í¬ê¸°ì˜ Rank-2 í…ì„œë¡œ ì¸ì½”ë”©ëœ ë²¡í„°ì˜ ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ ë°›ëŠ”ë‹¤.

timestepì„ ê¸°ì¤€ìœ¼ë¡œ ìˆœíšŒí•˜ë©° ê° timestep tì—ì„œ í˜„ì¬ ìƒíƒœì™€ ì…ë ¥ì„ ì—°ê²°í•˜ì—¬ ì¶œë ¥ì„ ê³„ì‚°í•œë‹¤. ì´í›„ ì¶œë ¥ì´ ë‹¤ìŒ ìŠ¤í…œì˜ ìƒíƒœë¡œ ì„¤ì •í•œë‹¤.

ë‹¹ì—°íˆ ì²« íƒ€ì„ìŠ¤íƒ­ì€ ì´ì „ ì¶œë ¥ì´ ì •ì˜ëœ ê²ƒì´ ì—†ìœ¼ë‹ˆ í˜„ì¬ ìƒíƒœê°€ ì—†ë‹¤. ì´ë• ë„¤íŠ¸ì›Œí¬ **ì´ˆê¸° ìƒíƒœ**ì¸ 0ë²¡í„°ë¡œ ìƒíƒœë¥¼ ì´ˆê¸°í™” í•œë‹¤.

> ì™œ 0ë²¡í„°ë¡œ ì´ˆê¸°í™”?   
> 1. ê·¸ëƒ¥ ê°€ì¥ ìì—°ìŠ¤ëŸ¬ìš´ ì„ íƒì´ë‹¤.
> 2. Gradient flow : ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ ë•Œë¬¸ì— í›ˆë ¨ ê³¼ì •ì—ì„œ ê¸°ìš¸ê¸° ì†Œì‹¤ ê°™ì€ ë¬¸ì œ ë°œìƒ ë°©ì§€
> 3. ë‹¤ì–‘í•œ ì…ë ¥ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ì¼ë°˜ì„±ì„ ê°™ë„ë¡ í•˜ê¸° ìœ„í•¨.

~~~python
# RNNì˜ pesudocode

state_t = 0 # íƒ€ì„ìŠ¤íƒ­ tì˜ ìƒíƒœ

for input_t in input_sequence: # ì‹œí€€ìŠ¤ì˜ ì›ì†Œ ë°˜ë³µ
    output_t = f(input_t, state_t)
    state_t = output_t # ì¶œë ¥ì€ ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•œ ìƒíƒœ

~~~

f í•¨ìˆ˜ëŠ” ì…ë ¥ê³¼ ìƒíƒœë¥¼ ì¶œë ¥ìœ¼ë¡œ ë°˜í™˜í•œë‹¤. ì´ë¥¼ 2ê°œì˜ í–‰ë ¬ W, U ê·¸ë¦¬ê³  biasë¥¼ ì‚¬ìš©í•˜ëŠ” ë³€í™˜ìœ¼ë¡œ ë³€ê²½í•  ìˆ˜ ìˆë‹¤.

~~~python
# ì¢€ ë” ìì„¸í•œ RNNì˜ pesudocode

state_t = 0 # íƒ€ì„ìŠ¤íƒ­ tì˜ ìƒíƒœ

for input_t in input_sequence: # ì‹œí€€ìŠ¤ì˜ ì›ì†Œ ë°˜ë³µ
    output_t = activation(dot(W, input_t) + dot(U, state_t) + b)
    state_t = output_t # ì¶œë ¥ì€ ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•œ ìƒíƒœ

~~~


```python
# ë„˜íŒŒì´ë¡œ êµ¬í˜„í•œ ê°„ë‹¨í•œ RNN
import numpy as np

timesteps = 100 # ì…ë ¥ ì‹œí€€ìŠ¤ì— ìˆëŠ” íƒ€ì„ìŠ¤íƒ­ì˜ ìˆ˜
input_features = 32 # ì…ë ¥ íŠ¹ì„±ì˜ ì°¨ì›
output_features = 64 # ì¶œë ¥ íŠ¹ì„±ì˜ ì°¨ì›
inputs = np.random.random((timesteps, input_features)) # ì…ë ¥ ë°ì´í„°: ì˜ˆì œë¥¼ ìœ„í•œ ëœë¤í•œ ì¡ìŒ
state_t = np.zeros((output_features,)) # ì´ˆê¸° ìƒíƒœ: 0ë²¡í„°

W = np.random.random((output_features, input_features)) # ëœë¤í•œ ê°€ì¤‘ì¹˜ í–‰ë ¬
U = np.random.random((output_features, output_features))
b = np.random.random((output_features,))

successive_outputs = []
for input_t in inputs: # (input_features, ) í¬ê¸°ì˜ ë²¡í„°
    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b) # ì…ë ¥ê³¼ í˜„ì¬ ìƒíƒœ(ì´ì „ ì¶œë ¥)ì„ ì—°ê²°í•˜ì—¬ í˜„ì¬ ì¶œë ¥ì„ ì–»ëŠ”ë‹¤.
    # tanh í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ë¹„ì„ í˜•ì„±ì„ ì¶”ê°€í•¨

    successive_outputs.append(output_t) # ì¶œë ¥ì„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
    state_t = output_t # ë‹¤ìŒ íƒ€ì„ìŠ¤íƒ­ì„ ìœ„í•´ ë„¤íŠ¸ì›Œí¬ì˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.

final_output_sequence = np.stack(successive_outputs, axis = 0) # ìµœì¢… ì¶œë ¥ì€ (timesteps, output_features) í¬ê¸°ì˜ Rank-2 í…ì„œë‹¤.

```

ì±…ì—ì„œ ì•„ì£¼ ì‰½ë„¤ìš”ë¼ê³  í•¨.

RNNì€ ë°˜ë³µí•  ë•Œ ì´ì „ì— ê³„ì‚°í•œ ì •ë³´ë¥¼ ì¬ì‚¬ìš©í•˜ëŠ” for ë£¨í”„ì— ì§€ë‚˜ì§€ ì•ŠëŠ”ë‹¤.

ë¬¼ë¡  ì´ ì •ì˜ì— ë§ëŠ” RNNì˜ ì¢…ë¥˜ëŠ” ë§ë‹¤. ì§€ê¸ˆì€ ì–´ë””ê¹Œì§€ë‚˜ ì§„ì§œ ë„ˆë¬´ ë§¤ìš° ì™„ì „ ê°„ë‹¨í•œ RNNì˜ í˜•íƒœì¼ ë¿.

![6](/assets/img/study/deep/ch10/2.png)

> ë°©ê¸ˆ ì˜ˆì‹œì—ì„œ ìµœì¢… ì¶œë ¥ì€ (timesyeps, output_feaures) í¬ê¸°ì˜ Rank-2 í…ì„œë‹¤. ê° íƒ€ì„ìŠ¤í…ì€ ì‹œê°„ tì—ì„œ ì¶œë ¥ì„ ë‚˜íƒ€ë‚¸ë‹¤. ì¶œë ¥ í…ì„œì˜ ê° íƒ€ì„ìŠ¤í… tëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì— ìˆëŠ” íƒ€ì„ìŠ¤í… 0 ~ t ê¹Œì§€ ì „ì²´ ê³¼ê±°ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ ë§ì€ ê²½ìš° ì „ì²´ ì¶œë ¥ì˜ ì‹œí€€ìŠ¤ê°€ í•„ìš”ì—†ë‹¤.  
> ì „ì²´ ì‹œí€€ìŠ¤ì˜ ì •ë³´ë¥¼ ì•Œê³  ìˆìœ¼ë‹ˆ ë§ˆì§€ë§‰ ì¶œë ¥ output_t ë§Œìˆìœ¼ë©´ ë¨



### ì¼€ë¼ìŠ¤ì˜ ìˆœí™˜ ì¸µ
---

ë°©ê¸ˆ ì˜ˆì œê°€ SimpleRNNì´ë‹¤.

ì´ë„˜ì€ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ê°€ ì•„ë‹ˆë¼ ì¼€ë¼ìŠ¤ì˜ ë‹¤ë¥¸ ì¸µê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì‹œí€€ìŠ¤ì˜ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•œë‹¤.

ì—„ë°€íˆ ë”°ì§€ë©´ ì…ë ¥ í¬ê¸°ëŠ” (batch_size, timesteps, input_features) í¬ê¸°ì˜ ì…ë ¥ì„ ë°›ëŠ”ë‹¤.

ì‹œì‘í•  ë•Œ Input() í•¨ìˆ˜ì˜ shape ë§¤ê°œë³€ìˆ˜ì— timesteps í•­ëª©ì„ Noneìœ¼ë¡œ ì§€ì •í•  ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì„ì˜ì˜ ê¸¸ì´ë¥¼ ê°€ì§„ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.

> ì•„ë§ˆ ì»´í“¨í„° ë¹„ì „ ì²« ì¥ì—ì„œ ìœ ì‚¬í•˜ê²Œ batch ì‚¬ì´ì¦ˆë¥¼ ìœ ë™ì ìœ¼ë¡œ ì§€ì •í–ˆë˜ ì ì´ ìˆì„ ê²ƒì„.




```python
# ì–´ë–¤ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” RNN
from tensorflow import keras
from tensorflow.keras import layers

num_features = 14
inputs = keras.Input(shape=(None, num_features))
outputs = layers.SimpleRNN(16)(inputs)

print(outputs.shape)
```

ëª¨ë¸ì´ ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•´ì•¼ í•œë‹¤ë©´ íŠ¹íˆ ìœ ìš©í•˜ë‹¤. í•˜ì§€ë§Œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ëª¨ë‘ ê°™ë‹¤ë©´ ì™„ì „í•œ ì…ë ¥ í¬ê¸°ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.

> ëª¨ë¸ ìš”ì•½ ì •ë³´ì—ì„œ ê¸¸ì´ ì •ë³´ë„ ì œê³µë°›ì„ ìˆ˜ ìˆìœ¼ë©° ì¼ë¶€ ì„±ëŠ¥ ìµœì í™”ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŒ





```python
# ë§ˆì§€ë§‰ ì¶œë ¥ ìŠ¤íƒ­ë§Œ ë°˜í™˜í•˜ëŠ” RNN ì¸µ

num_features = 14
steps = 120
inputs = keras.Input(shape=(steps, num_features))
outputs = layers.SimpleRNN(16, return_sequences=False)(inputs) # return_sequences=Falseê°€ ê¸°ë³¸ ê°’
print(outputs.shape)
```


```python
# ì „ì²´ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” RNN ì¸µ

num_features = 14
steps = 120
inputs = keras.Input(shape=(steps, num_features))
outputs = layers.SimpleRNN(16, return_sequences=True)(inputs)
print(outputs.shape)
```

ë„¤íŠ¸ì›Œí¬ í‘œí˜„ë ¥ì„ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°œì˜ ìˆœí™˜ ì¸µì„ ì°¨ë¡€ë•Œë¡œ ìŒ“ëŠ” ê²ƒì´ ìœ ìš©í•  ë•Œê°€ ìˆë‹¤.

ì´ ë•Œ ì¤‘ê°„ì¸µë“¤ì´ ì „ì²´ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•˜ë„ë¡ í•´ì•¼í•¨.


```python
# ìŠ¤íƒœí‚¹(stacking) RNN ì¸µ

inputs = keras.Input(shape=(steps, num_features))
x = layers.SimpleRNN(16, return_sequences=True)(inputs)
x = layers.SimpleRNN(16, return_sequences=True)(x)
outputs = layers.SimpleRNN(16)(x)
```

ì‹¤ì œë¡œëŠ” SimpleRNN ì¸µì„ ê±°ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì‹¤ì „ì— ì“°ê¸°ì—” ë„ˆë¬´ ë‹¨ìˆœí•˜ë‹¤.

ì¸µì´ ë§ì€ ì¼ë°˜ì  ë„¤íŠ¸ì›Œí¬ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•œ í˜„ìƒì¸ ê·¸ë ˆì´ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œ ë•Œë¬¸ì´ë‹¤.

ì´ëŸ° ë¬¸ì œë¥¼ ìœ„í•´ ì¼€ë¼ìŠ¤ëŠ” LSTMê³¼ GRU ì¸µì´ ìˆë‹¤.

ìš°ì„  LSTMë¶€í„° ë³´ì ì´ë†ˆì€ SimpleRNNì˜ ë³€ì¢…ìœ¼ë¡œ ì •ë³´ë¥¼ ì—¬ëŸ¬ íƒ€ì„ìŠ¤í…œì— ê±¸ì³ ë‚˜ë¥´ëŠ” ë°©ì‹ì´ ì¶”ê°€ëœë‹¤. ì²˜ë¦¬í•  ì‹œí€€ìŠ¤ì— ë‚˜ë€íˆ ì‘ë™í•˜ëŠ” ì»¨ë² ì´ì–´ ë²¨íŠ¸ë¥¼ ìƒê°í•´ë³´ë¼

ì‹œí€€ìŠ¤ì˜ ì–´ëŠ ì§€ì ì—ì„œ ì¶”ì¶œëœ ì •ë³´ê°€ ì»¨ë² ì´ì–´ ë²¨íŠ¸ ìœ„ë¡œ ì˜¬ë¼ê°€ í•„ìš”í•œ ì‹œì ì˜ íƒ€ì„ìŠ¤í…œìœ¼ë¡œ ì´ë™í•˜ì—¬ ë–¨êµ½ë‹ˆë‹¤. ì´ê²Œ LSTMì´ í•˜ëŠ” ì¼ì…ë‹ˆë‹¤. ë‚˜ì¤‘ì„ ìœ„í•´ ì •ë³´ë¥¼ ì €ì¥í•¨ìœ¼ë¡œì¨ ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì˜¤ë˜ëœ ì‹œê·¸ë„ì´ ì ì°¨ ì†Œì‹¤ë˜ëŠ” ê²ƒì„ ë§‰ì•„ì¤€ë‹¤.

> ì”ì°¨ ì—°ê²°ê³¼ ë¹„ìŠ·í•¨

ìì„¸íˆ ì‚´í´ë³´ê¸° ìœ„í•´ SimpleRNN ì…€ ê·¸ë¦¼ì„ ë³´ì  
ê°€ì¤‘ì¹˜ í–‰ë ¬ ì—¬ëŸ¬ ê°œê°€ ë‚˜ì˜¤ë¯€ë¡œ ì¶œë ¥ì„ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ì 0ìœ¼ë¡œ ì…€ì— ìˆëŠ” W, U í–‰ë ¬ì„ í‘œí˜„í•¨.

![7](/assets/img/study/deep/ch10/3.png)

ë‹¤ìŒ ê·¸ë¦¼ì€ íƒ€ì„ìŠ¤í…ì„ ê°€ë¡œì§ˆëŸ¬ ì •ë³´ë¥¼ ë‚˜ë¥´ëŠ” ë°ì´í„° íë¦„ì„ ì¶”ê°€ë¡œ ë³´ì¸ë‹¤. í‹°ì„ìŠ¤í… tì—ì„œ ì´ ê°’ì„ ì´ë™ ìƒíƒœ c_t(carry)ë¼ê³  ë¶€ë¥´ê² ë‹¤. ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…€ì´ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½ëœë‹¤.

![8](/assets/img/study/deep/ch10/4.png)

ì´ë™ ìƒíƒœëŠ” ì…ë ¥ ì—°ê²°ê³¼ ìˆœí™˜ ì—°ê²°(ìƒíƒœ)ì— ì—°ê²°ëœë‹¤. ê·¸ëŸ° í›„ ë‹¤ìŒ íƒ€ì„ìŠ¤í…ìœ¼ë¡œ ì „ë‹¬ë  ìƒíƒœì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.

> ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëƒ?   
> activation(c_t) * activation(W.input + U.state + b) í˜•íƒœë¡œ

ê°œë…ì ìœ¼ë¡œ ë³´ë©´ ë°ì´í„°ë¥¼ ì‹¤ì–´ ë‚˜ë¥´ëŠ” ì´ íë¦„ì´ ë‹¤ìŒ ì¶œë ¥ê³¼ ìƒíƒœë¥¼ ì¡°ì ˆí•œë‹¤.

ê·¸ëŸ¼ ë‹¤ìŒ ì´ë™ ìƒíƒœ c_t+1ì´ ê³„ì‚°ë˜ëŠ” ë°©ì‹ì´ë‹¤.

ì§€ê¸ˆ ê·¸ë¦¼ì—ëŠ” 3ê°œì˜ ë‹¤ë¥¸ ë³€í™˜ì´ ê´€ë ¨ë˜ì–´ìˆë‹¤. (ì •ì‚¬ê²©í˜• 3ê°œ) ì´ë¥¼ ê°ê° i, f, kë¼ ë¶€ë¥´ê³  ì•„ë˜ ì˜ì‚¬ ì½”ë“œë¥¼ ë³´ë¼

~~~python
output_t  = activation(c_t) * activation(dot(input_t, W0) + dot(state_t, Uo) + b0)

i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)
f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)
~~~
i, f, kë¥¼ ê²°í•©í•´ì„œ ìƒˆë¡œìš´ ì´ë™ ìƒíƒœ c_t+1ì„ êµ¬í•œë‹¤.

~~~python
c_t+1 = i_t * k_t + c_t * f_t
~~~

ë” ìì„¸í•˜ê²Œ ì„¤ëª…ì„ í•˜ë©´

![9](/assets/img/study/deep/ch10/5.png)

ì—°ì‚°ë“¤ì´ í•˜ëŠ” ì¼ì„ í•´ì„í•˜ë©´ ê° ì˜ë¯¸ì— ëŒ€í•´ í†µì°°ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ **c_tì™€ f_tì˜ ê³±ì…ˆì€ ì´ë™ì„ ìœ„í•œ ë°ì´í„° íë¦„ì—ì„œ ê´€ë ¨ì´ ì ì€ ì •ë³´ë¥¼ ì˜ë„ì ìœ¼ë¡œ ì‚­ì œí•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.**

í•œí¸ i_tì™€ k_tëŠ” í˜„ì¬ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³  ì´ë™ íŠ¸ë™ì„ ìƒˆë¡œìš´ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. **í•˜ì§€ë§Œ ê²°êµ­ ì´ëŸ° í•´ì„ì€ í° ì˜ë¯¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ ì—°ì‚°ë“¤ì´ ì‹¤ì œë¡œ í•˜ëŠ” ì¼ì€ ì—°ì‚°ì— ê´€ë ¨ëœ ê°€ì¤‘ì¹˜ í–‰ë ¬ì— ë”°ë¼ ê²°ì •ë˜ê¸° ë•Œë¬¸ì´ë‹¤.**

ì´ ê°€ì¤‘ì¹˜ëŠ” end to end ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœë‹¤. í›ˆë ¨ ë°˜ë³µë§ˆë‹¤ ë§¤ë²ˆ ìƒˆë¡œ ì‹œì‘ë˜ë©° ì´ëŸ°ì €ëŸ° ì—°ì‚°ë“¤ì— íŠ¹ì • ëª©ì ì„ ë¶€ì—¬í•˜ê¸°ê°€ ë¶ˆê°€ëŠ¥í•˜ë‹¤.

RNN ì…€ì˜ êµ¬ì¡°ëŠ” ê°€ì„¤ ê³µê°„ì„ ê²°ì •í•œë‹¤. í›ˆë ¨í•  ë•Œ ì´ ê³µê°„ì—ì„œ ì¢‹ì€ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ”ë‹¤. ì…€ì˜ êµ¬ì¡°ê°€ ì…€ì´ í•˜ëŠ” ì¼ì„ ê²°ì •í•˜ì§€ ì•ŠëŠ”ë‹¤.

**ê°™ì€ ì…€ì´ë”ë¼ë„ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ëŠ” ê²½ìš° ë§¤ìš° ë‹¤ë¥¸ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.ë”°ë¼ì„œ RNN ì…€ì„ êµ¬ì„±í•˜ëŠ” ì—°ì‚° ì¡°í•©ì€ ì—”ì§€ë‹ˆì–´ë§ì ì¸ ì„¤ê³„ê°€ ì•„ë‹ˆë¼ ê°€ì„¤ ê³µê°„ì˜ ì œì•½ ì¡°ê±´ìœ¼ë¡œ í•´ì„í•˜ëŠ” ê²ƒì´ ë‚«ë‹¤.**

ìš”ì•½í•˜ë©´ LSTM ì…€ì˜ êµ¬ì²´ì ì¸ êµ¬ì¡°ì— ëŒ€í•´ ì´í•´í•  í•„ìš”ê°€ ì „í˜€ ì—†ë‹¤. ì´ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ìš°ë¦¬ê°€ í•´ì•¼ í•  ì¼ì´ ì•„ë‹ˆë‹¤. **LSTM ì…€ì˜ ì—­í• ë§Œ ê¸°ì–µí•˜ë©´ ëœë‹¤. ê³¼ê±° ì •ë³´ë¥¼ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì…ë ¥í•´ ê·¸ë ˆì´ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì´ë‹¤.**


## ìˆœí™˜ ì‹ ê²½ë§ì˜ ê³ ê¸‰ ì‚¬ìš©ë²•
----

ì•ì—ì„œ ì„¤ëª…í•œ ë‚´ìš©ì€

1. RNNì´ ë¬´ì—‡ì¸ì§€, RNNì˜ ì‘ë™ë°©ì‹
2. LSTMì´ ë¬´ì—‡ì¸ì§€ì™€ ë‹¨ìˆœí•œ RNNë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ì˜ ì²˜ë¦¬í•˜ëŠ” ì´ìœ 
3. ì¼€ë¼ìŠ¤ RNNì¸µì„ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•

ì´ì œ RNNì˜ ê³ ê¸‰ ê¸°ëŠ¥ì„ ì†Œê°œí•œë‹¤.

1. ìˆœí™˜ ë“œë¡­ì•„ì›ƒ : ë“œë¡­ì•„ì›ƒì˜ í•œ ì¢…ë¥˜ë¡œ ìˆœí™˜ ì¸µì—ì„œ ê³¼ëŒ€ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©
2. ìŠ¤íƒœí‚¹ ìˆœí™˜ ì¸µ : ëª¨ë¸ì˜ í‘œí˜„ ëŠ¥ë ¥ì„ ì¦ê°€ì‹œí‚´(ëŒ€ì‹  ê³„ì‚°ì´ ë§ì´ ìš”êµ¬ë¨)
3. ì–‘ë°©í–¥ ìˆœí™˜ ì¸µ : ìˆœí™˜ ë„¤íŠ¸ì›Œí¬ì— ê°™ì€ ì •ë³´ë¥¼ ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ì£¼ì…í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì´ê³  ê¸°ì–µì„ ì¢€ ë” ì˜¤ë˜ ìœ ì§€ì‹œí‚´.

ì˜¨ë„ ì˜ˆì¸¡ RNNì„ ë” ê°œì„ í•´ë³´ì



### ê³¼ëŒ€ì í•©ì„ ê°ì†Œí•˜ê¸° ìœ„í•´ ìˆœí™˜ ë“œë¡­ì•„ì›ƒ ì‚¬ìš©
----

ì•ì„œ ì‚¬ìš©í•œ LSTM ëª¨ë¸ì„ ë‹¤ì‹œ ì‚¬ìš©í•´ë³´ì. í›ˆë ¨ ì†ì‹¤ê³¼ ê²€ì¦ ì†ì‹¤ ê³¡ì„ ì„ ë³´ë©´ ëª¨ë¸ì´ ê³¼ëŒ€ì í•©ì¸ì§€ ì•Œ ìˆ˜ ìˆë‹¤.

ëª‡ ë²ˆì˜ ì—í¬í¬ ì´í›„ì— í›ˆë ¨ ì†ì‹¤ê³¼ ê²€ì¦ ì†ì‹¤ì´ í˜„ì €í•˜ê²Œ ë²Œì–´ì§€ê¸° ì‹œì‘í•œë‹¤. ì´ëŸ° í˜„ìƒì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë“œë¡­ì•„ì›ƒì„ ì‚¬ìš©í–ˆê³  ì´ë¯¸ ì‹¤ìŠµë„ í–ˆë‹¤.

ìˆœí™˜ ì‹ ê²½ë§ì— ë“œë¡­ì•„ì›ƒì„ ì ìš©í•˜ê¸°ì—” ê°„ë‹¨í•˜ì§€ ì•Šë‹¤.

ì´ë¯¸ ìˆœí™˜ ì¸µ ì´ì „ì— ë“œë¡­ì•„ì›ƒì„ ì ìš©í•˜ë©´ í•™ìŠµì— ë” ë°©í•´ë˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ì™”ë‹¤. 2016ë…„ ë°•ì‚¬ ë…¼ë¬¸ì—ì„œ ìˆœí™˜ ë„¤íŠ¸ì›Œí¬ì— ë“œë¡­ì•„ì›ƒì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ëƒˆë‹¤.  

> ì§€ë¦°ë‹¤..

íƒ€ì„ìŠ¤í…ë§ˆë‹¤ ëœë¤í•˜ê²Œ ë“œë¡­ì•„ì›ƒ ë§ˆìŠ¤í¬ë¥¼ ë°”ê¾¸ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë™ì¼í•œ ë“œë¡­ì•„ì›ƒ ë§ˆìŠ¤í¬(ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ ë“œë¡­ì•„ì›ƒ ì§„í–‰)ë¥¼ ëª¨ë“  íƒ€ì„ìŠ¤í…ì— ì ìš©í•´ì•¼ í•œë‹¤.

**`GRU`ë‚˜ `LSTM` ê°™ì€ ìˆœí™˜ ê²Œì´íŠ¸ì— ì˜í•´ ë§Œë“¤ì–´ì§€ëŠ” í‘œí˜„ì„ ê·œì œí•˜ë ¤ë©´ ìˆœí™˜ ì¸µ ë‚´ë¶€ ê³„ì‚°ì— ì‚¬ìš©ëœ í™œì„±í™” í•¨ìˆ˜ì— íƒ€ì„ìŠ¤í…œë§ˆë‹¤ ë™ì¼í•œ ë“œë¡­ì•„ì›ƒ ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•´ì•¼ í•œë‹¤.**(ìˆœí™˜ ë“œë¡­ ì•„ì›ƒ ë§ˆìŠ¤í¬).

ëª¨ë“  íƒ€ì„ìŠ¤í…ì— ë™ì¼í•œ ë“œë¡­ì•„ì›ƒ ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ë©´ ë„¤íŠ¸ì›Œí¬ê°€ í•™ìŠµ ì˜¤ì°¨ë¥¼ íƒ€ì„ìŠ¤í…œì— ê±¸ì³ ì ì ˆí•˜ê²Œ ì „íŒŒí•  ìˆ˜ ìˆë‹¤.

ì¼€ë¼ìŠ¤ì˜ ëª¨ë“  ìˆœí™˜ ì¸µì€ 2ê°œì˜ ë“œë¡­ì•„ì›ƒ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°–ëŠ”ë‹¤.  
`dropout`ì€ ì¸µì˜ ì…ë ¥ì— ëŒ€í•œ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ì„ ì •í•˜ëŠ” ë¶€ë™ ì†Œìˆ˜ì  ê°’ì„.   
`recurrent_dropout`ì€ ìˆœí™˜ ìƒíƒœì˜ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ì„ ì •í•œë‹¤.  

ì²« LSTM ì˜ˆì œì˜ LSTM ì¸µì— ìˆœí™˜ ë“œë¡­ì•„ì›ƒì„ ì ìš©í•´ ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì‚´í´ë³¸ë‹¤.  

**ë“œë¡­ì•„ì›ƒìœ¼ë¡œ ê·œì œëœ ë„¤íŠ¸ì›Œí¬ëŠ” ìˆ˜ë ´í•˜ëŠ” ë° ì–¸ì œë‚˜ ì˜¤ë˜ ê±¸ë¦°ë‹¤.**

> ì¦‰ í›ˆë ¨ ë°˜ë³µì´ ë§ì•„ì•¼í•¨.




```python
# ë“œë¡­ì•„ì›ƒ ê·œì œë¥¼ ì ìš©í•œ LSTM ëª¨ë¸ í›ˆë ¨í•˜ê³  í‰ê°€í•˜ê¸°
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf

inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))

x = layers.LSTM(32, recurrent_dropout=0.25)(inputs)
x = layers.Dropout(0.5)(x) # Dense ì¸µì— ê·œì œë¥¼ ì¶”ê°€í•˜ê¸° ìœ„í•´ LSTM ì¸µ ë’¤ì— dropout ì¶”ê°€
outputs = layers.Dense(1)(x)

with tf.device("/gpu:0"):
    model = keras.Model(inputs, outputs)

    callbacks = [
        keras.callbacks.ModelCheckpoint("jena_lstm_dropout.keras",
                                        save_best_only=True)
    ]

    model.compile(optimizer="rmsprop",
                loss="mse",
                metrics=["mae"])

    history = model.fit(train_dataset,
                        epochs = 50,
                        validation_data = val_dataset,
                        callbacks = callbacks)
```

![10](/assets/img/study/deep/ch10/6.png)

<div align=center>
ë“œë¡­ì•„ì›ƒì„ ì ìš©í•œ LSTM ëª¨ë¸ì˜ í›ˆë ¨ê³¼ ê²€ì¦ ì†ì‹¤
</div>

ê·¸ë¦¼ì„ ë³´ë©´ ì–´ì¨‹ë“  GD ì•Œê³ ë¦¬ì¦˜ ë•Œë¬¸ì— training ì—ëŸ¬ëŠ” ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³´ì„.

ê·¼ë°, ì¤‘ìš”í•œê±´ validation ì—ëŸ¬ê°€ ê°ì†Œí•˜ë‹¤ê°€ ê¾¸ì¤€íˆ ì¦ê°€í–ˆë‹¤ëŠ” ëª¨ìŠµì— ì§‘ì¤‘í•´ì•¼í•¨.

> ëª¨ë¸ì˜ ê²€ì¦ ì†ì‹¤ì´ ê°ì†Œí•˜ë‹¤ê°€ ì¦ê°€í•œ êµ¬ê°„ : ê³¼ëŒ€ì í•© ë°œìƒ!


ì´ê²Œ ë­” ë§ì´ëƒ? ê¸°ì¶œ ë¬¸ì œëŠ” ì˜ í’€ë‹¤ê°€ ëª¨ì˜ê³ ì‚¬ë¥¼ ì¡°ì§€ëŠ” ê²ƒ.
**ì¦‰, ê³¼ëŒ€ì í•©**

> ê·¼ë° ì±…ì—ì„œëŠ” ì¼ë‹¨ ë”ì´ìƒ ê³¼ëŒ€ì í•©ì€ ì—†ë‹¤ê³  ë§í•˜ê¸´í•¨. ë³´ëŠ” ëˆˆê³¼ ê´€ì ì˜ ì°¨ì´ì¸ë“¯?(ê¹ƒí—ˆë¸Œ ì˜ˆì œ ì½”ë“œ ê²°ê³¼ëŠ” ê³¼ëŒ€ì í•©ì„ ã…‹)

**[ê³¼ì í•© ì •ë¦¬]**

- Training lossì™€ validation(or test) lossê°€ ê°™ì´ ê°ì†Œí•˜ëŠ” êµ¬ê°„ (underfitting).  
- Training lossëŠ” ê°ì†Œí•˜ì§€ë§Œ, validation(or test) lossëŠ” ì¦ê°€í•˜ëŠ” êµ¬ê°„ (overfitting).

**[RNN ëŸ°íƒ€ì„ ì„±ëŠ¥]**

ì´ ì¥ì— ìˆëŠ” ëª¨ë¸ì²˜ëŸ¼ íŒŒë¼ë¯¸í„° ê°œìˆ˜ê°€ ë§¤ìš° ì ì€ ìˆœí™˜ ì‹ ê²½ë§ì€ GPUë³´ë‹¤ ë©€í‹°ì½”ì–´ CPUì—ì„œ ë¹ ë¥¸ ê²½í–¥ì´ ìˆë‹¤.

> ë‘˜ë‹¤ ê²ë‚˜ ë‹µë‹µí•˜....

ì‘ì€ í–‰ë ¬ ê³±ì…ˆë§Œ í¬í•¨í•´ì„œ for ë£¨í”„ ë•Œë¬¸ì— ì—°ì†ëœ ê³±ì…ˆì´ ì˜ ë³‘ë ¬í™”ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.

ê¸°ë³¸ ë§¤ê°œë³€ìˆ˜ë¡œ ì„¤ì •ëœ ì¼€ë¼ìŠ¤ LSTMê³¼ GRU ì¸µì„ GPUì—ì„œ ì‚¬ìš©í•  ë•Œ cuDNN ì»¤ë„ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤. cuDNNì€ ì—”ë¹„ë””ì•„ê°€ ì œê³µí•˜ëŠ” ê³ ë„ë¡œ ìµœì í™”ëœ ì €ìˆ˜ì¤€ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ì´ë‹¤.(ì´ì „ ì¥ì—ì„œ ì–¸ê¸‰í–ˆë‹¤ê³  í•¨)

ëŠ˜ ê·¸ë ‡ë“¯ cuDNN ì»¤ë„ì€ ë¹ ë¥´ì§€ë§Œ ìœ ì—°í•˜ì§€ ëª»í•œ ì¥ì ì„ ê°€ì§„ë‹¤. ê¸°ë³¸ ì»¤ë„ì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ìˆ˜í–‰í•˜ë ¤ë©´ ì†ë„ê°€ í¬ê²Œ ëŠë ¤ì§€ëŠ” ê²ƒì„ ê²½í—˜í•  ê²ƒì´ë‹¤.  

ë•Œë¬¸ì— ì—”ë¹„ë””ê°€ì•„ê°€ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.  
ì˜ˆë¥¼ ë“¤ì–´, LSTM, GRU cuDNN ì»¤ë„ì€ ìˆœí™˜ ë“œë¡­ì•„ì›ƒì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ì¸µì— ìˆœí™˜ ë“œë¡­ì•„ì›ƒì„ ì¶”ê°€í•˜ë©´(ê³„ì‚° ë¹„ìš©ì€ ë™ì¼)ì¼ë°˜ì ìœ¼ë¡œ GPUë³´ë‹¤ 2 ~ 5ë°° ëŠë¦° ì¼ë°˜ í…ì„œë¡œ êµ¬í˜„ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.

cuDNNì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œ RNN ì¸µì˜ ì†ë„ë¥¼ ë†’ì´ëŠ” ë°©ë²•ìœ¼ë¡œ ì¸µì„ **ì–¸ë¡¤ë§(unrolling)**í•  ìˆ˜ ìˆë‹¤. for ë£¨í”„ë¥¼ ì–¸ë¡¤ë§í•˜ë©´ ë£¨í”„ë¥¼ ì œê±°í•˜ê³  ë£¨í”„ì˜ ë‚´ìš©ì„ ë‹¨ìˆœíˆ Në²ˆ ê¸°ìˆ í•œë‹¤. RNNì˜ for ë£¨í”„ì˜ ê²½ìš° ì–¸ë¡¤ë§í•˜ë©´ í…ì„œí”Œë¡œê°€ ê³„ì‚° ê·¸ë˜í”„ë¥¼ ìµœì í™”í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤.

í•˜ì§€ë§Œ RNN ë©¤ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìƒë‹¹íˆ ì¦ê°€ì‹œí‚¨ë‹¤.
> ì´ê²Œ ì§€ê¸ˆì€ ê·¸ë‹¥ ìƒê´€ì—†ì§€ë§Œ, ì‹¤ì‹œê°„ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš° í˜¹ì€ ì—£ì§€ ì»´í“¨íŒ…ì— ì‚¬ìš©ë  ê²½ìš°ì—” ì•„ì£¼ ì¹˜ëª…ì 

ë”°ë¼ì„œ ë¹„êµì  ì‘ì€ ì‹œí€€ìŠ¤ì—ë§Œ ê°€ëŠ¥í•˜ë‹¤. ë˜í•œ, ëª¨ë¸ì´ ë°ì´í„°ì— ìˆëŠ” íƒ€ì„ìŠ¤í… ìˆ˜ë¥¼ ë¯¸ë¦¬ ì•Œ ìˆ˜ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.

> Input()í•¨ìˆ˜ì— ì „ë‹¬í•˜ëŠ” shapeê°€ None ì•„ë‹Œ ê²½ìš°

```python
inputs = keras.Input(shape=(sequence_length, num_features))
x = layers.LSTM(32, recurrent_dropout=0.2, unrool=True)(inputs)
```

ì°¸ê³ ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì—ì„œ cuDNN ì»¤ë„ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

The requirements to use the cuDNN implementation are:

activation == tanh  
recurrent_activation == sigmoid  
recurrent_dropout == 0  
unroll is False  
use_bias is True  
Inputs, if use masking, are strictly right-padded.  
Eager execution is enabled in the outermost context.  


### ìŠ¤íƒœí‚¹ ìˆœí™˜ ì¸µ
-----

ì„±ëŠ¥ìƒ ë³‘ëª©ì´ ìˆëŠ” ê²ƒ ê°™ìœ¼ë¯€ë¡œ ë„¤íŠ¸ì›Œí¬ì˜ ìš©ëŸ‰ê³¼ í‘œí˜„ë ¥ì„ ëŠ˜ë ¤ì•¼í•¨.

ì´ì „ì— ìš°ë¦¬ê°€ ë°°ì› ë˜ ë‚´ìš© ì¤‘

1. ê³¼ëŒ€ì í•©ì„ ì¤„ì´ì!
2. ê³¼ëŒ€ì í•©ì´ ì¼ì–´ë‚  ë•Œê¹Œì§€ ëª¨ë¸ì˜ ìš©ëŸ‰ì„ ëŠ˜ë¦¬ì

ë­ ì´ëŸ° ê³¼ì •ì„ í•œë²ˆ í–ˆìŒ. 5ì¥ì¸ê°€??

ë„¤íŠ¸ì›Œí¬ì˜ ìš©ëŸ‰ì„ ëŠ˜ë¦¬ë ¤ë©´ ë‹¹ì—°íˆ ì¸µì— ìˆëŠ” ìœ ë‹›ì˜ ìˆ˜ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ì¸µì„ ë” ë§ì´ ì¶”ê°€í•˜ë©´ ë¨.

ìˆœí™˜ ìŠ¤íƒœí‚¹ì€ ë” ê°•ë ¥í•œ ìˆœí™˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“œëŠ” ê³ ì „ì ì¸ ë°©ë²•ì´ë‹¤.

ì¼€ë¼ìŠ¤ì—ì„œ ìˆœí™˜ ì¸µì„ ì°¨ë¡€ëŒ€ë¡œ ìŒ“ìœ¼ë ¤ë©´ ëª¨ë“  ì¤‘ê°„ì¸µì€ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ì¶œë ¥ë§Œ ì•„ë‹ˆê³  ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ì¶œë ¥í•´ì•¼ í•œë‹¤.

> retrun_sequences = True

ì´ë²ˆì—” `GRU(Gated Recuurent unit)`ì„ ì‚¬ìš©í•œë‹¤. ì´ëŠ” LSTMì˜ ê°„ì†Œí™” ë²„ì „ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤.


```python
#ë“œë¡­ì•„ì›ƒ ê·œì œì™€ ìŠ¤íƒœí‚¹ì„ ì ìš©í•œ GRU ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  í‰ê°€í•˜ê¸°

inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
# í›ˆë ¨ ì†ë„ë¥¼ ë†“ì´ê¸° ìœ„í•´ ìˆœí™˜ ë“œë¡­ì•„ì›ƒì„ ì œì™¸í•©ë‹ˆë‹¤.
# x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)
# x = layers.GRU(32, recurrent_dropout=0.5)(x)
x = layers.GRU(32, return_sequences=True)(inputs)
x = layers.GRU(32)(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_stacked_gru_dropout.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=50,
                    validation_data=val_dataset,
                    callbacks=callbacks)
model = keras.models.load_model("jena_stacked_gru_dropout.keras")
print(f"í…ŒìŠ¤íŠ¸ MAE: {model.evaluate(test_dataset)[1]:.2f}")

```

ì¸µì„ ì¶”ê°€í•´ì„œ ê·¹ì ìœ¼ë¡œ ë­”ê°€ ìƒê¸´ê±´ ì•„ë‹ˆì§€ë§Œ ì¡°ê¸ˆ í–¥ìƒì‹œì¼°ìŒ.

ê·¸ë˜ì„œ ë¹„ìš©ì„ ë”°ì ¸ë´¤ì„ ë•Œ ë„¤íŠ¸ì›Œí¬ ìš©ëŸ‰ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.

### ì–‘ë°©í–¥ RNN ì‚¬ìš©í•˜ê¸°
----

**ì–‘ë°©í–¥ RNN(Bidirectional RNN)** ì€ RNNì˜ ë³€ê²½ì¢…ì´ê³  íŠ¹ì • ì‘ì—…ì—ì„œ ê¸°ë³¸ RNNë³´ë‹¤ í›¨ì”¬ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ…ë‹ˆë‹¤. ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” ë§¥ê°€ì´ë²„ ì¹¼ì´ë¼ê³  í•  ì •ë„ë¡œ ì¦ê²¨ì„œ ì‚¬ìš©í•œë‹¤ê³  í•¨.

RNNì€ íŠ¹íˆ ìˆœì„œì— ë¯¼ê°í•˜ë‹¤. íƒ€ì„ìŠ¤í…ì„ ì„ê±°ë‚˜ ê±°ê¾¸ë¡œ í•˜ë©´ rNNì´ ì‹œí€€ìŠ¤ì—ì„œ í•™ìŠµí•˜ëŠ” í‘œí˜„ì„ ì™„ì „íˆ ë°”ê¾¸ì–´ ë²„ë¦°ë‹¤.

ì–‘ë±¡í–¥ RNNì€ ì´ëŸ¬í•œ íŠ¹ì§•ì„ ì´ìš©í•œë‹¤. GRU, LSTM ê°™ì€ RNNì„ 2ê°œ ì‚¬ìš©í•œë‹¤. ê° RNNì€ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ í•œ ë°©í–¥ìœ¼ë¡œ ì²˜ë¦¬í•œ í›„ ê° í‘œí˜„ì„ í•©ì¹œë‹¤.  
ì‹œí€€ìŠ¤ë¥¼ ì–‘ìª½ ë°©í–¥ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— ì–‘ë°©í–¥ RNNì€ ë‹¨ë°©í–¥ RNNì´ ë†“ì¹˜ê¸° ì‰¬ìš´ íŒ¨í„´ì„ ê°ì§€í•  ìˆ˜ ìˆë‹¤.  

ìš°ì„  ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì‹œê°„ ì°¨ì›ì— ë”°ë¼ ê±°ê¾¸ë¡œ ìƒì„±í•˜ëŠ” ë°ì´í„° ì œë„ˆë ˆì´í„°ë¥¼ ë§Œë“¤ì–´ì•¼í•¨.


```python
def train_generator():
    while True:
        for samples, targets in train_dataset:
            yield samples[:, ::-1, :], targets

def val_generator():
    while True:
        for samples, targets in val_dataset:
            yield samples[:, ::-1, :], targets

train_gen = train_generator()
val_gen = val_generator()
```

`yield`ë¥¼ ì²˜ìŒ ë³¸ë‹¤ë©´ [ì—¬ê¸°](https://dojang.io/mod/page/view.php?id=2412)ë¥¼ ì°¸ê³  ë°”ëŒ.


```python
inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
# í›ˆë ¨ ì†ë„ë¥¼ ë†“ì´ê¸° ìœ„í•´ ìˆœí™˜ ë“œë¡­ì•„ì›ƒì„ ì œì™¸í•©ë‹ˆë‹¤.
# x = layers.LSTM(32, recurrent_dropout=0.25)(inputs)
x = layers.LSTM(32)(inputs)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_gen,
                    epochs=10,
                    steps_per_epoch=819,
                    validation_data=val_gen,
                    validation_steps=410)
```


```python
loss = history.history["mae"]
val_loss = history.history["val_mae"]
epochs = range(1, len(loss) + 1)
plt.figure()
plt.plot(epochs, loss, "bo", label="Training MAE")
plt.plot(epochs, val_loss, "b", label="Validation MAE")
plt.title("Training and validation MAE")
plt.legend()
plt.show()
```


    
![png](/assets/img/study/deep/ch10/output_63_0.png)
    


ìˆœì„œë¥¼ ë’¤ì§‘ì€ LSTMì€ ì´ì „ì— ìƒì‹ìœ¼ë¡œ ì¸¡ì •í–ˆë˜ ê¸°ì¤€ë³´ë‹¤ ë‚®ë‹¤.  

ì´ëŸ° ê²½ìš° ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.  
LSTMì€ ë¨¼ ê³¼ê±°ë³´ë‹¤ ìµœê·¼ ë‚´ìš©ì„ ë” ì˜ ê¸°ì–µí•œë‹¤. ë”°ë¼ì„œ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ê°€ ê±°ê¾¸ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒë³´ë‹¤ ì„±ëŠ¥ì´ ë†’ì•„ì•¼ë§Œ í•œë‹¤.

> ì˜ˆì œì—ì„œ ì‚¬ìš©í•œ ë°ì´í„°ëŠ” ì‹œê°„ì˜ ìˆœì„œê°€ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì— ì—­ìˆœìœ¼ë¡œ ì…ë ¥ì„ ì²˜ë¦¬í•˜ëŠ” ê²½ìš° ë‹¹ì—°íˆ ì‚¬ê³ ë‹¤

ìì—°ì–´ ì²˜ë¦¬ì˜ ê²½ìš°ëŠ” ì¢€ ë‹¤ë¥´ë‹¤. ë¬¸ì¥ì„ ì´í•´í•˜ëŠ” ë° ë‹¨ì–´ì˜ ì¤‘ìš”ì„±ì€ ìœ„ì¹˜ì— ë”°ë¼ ê²°ì •ë˜ì§€ ì•ŠëŠ”ë‹¤.

ê±°ê¾¸ë¡œ ëœ ì‹œí€€ìŠ¤ì—ì„œ í›ˆë ¨í•œ RNNì€ ì›ë˜ ì‹œí€€ìŠ¤ì—ì„œ í›ˆë ¨í•œ ê²ƒê³¼ëŠ” ë‹¤ë¥¸ í‘œí˜„ì„ í•™ìŠµí•œë‹¤.

> í‘œí˜„ì´ ë§ì´ ë‹¤ë¥¼ìˆ˜ë¡ ë” ì¢‹ìŒ

13ì¥ì— `ì•™ìƒë¸”(ensemble)` ê°œë…ì—ì„œ ìì„¸íˆ ë‚˜ì˜¨ë‹¤.

ì–‘ë°©í–¥ RNNì€ ì´ ì•„ì´ë””ì–´ë¥¼ ì‚¬ìš©í•´ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ëŠ” RNNì˜ ì„±ëŠ¥ì„ ë†’ì˜€ë‹¤.

![11](/assets/img/study/deep/ch10/7.png)

ì¼€ë¼ìŠ¤ëŠ” `Bidirectional()`ì„ ì´ìš©í•´ì„œ ì–‘ë°©í–¥ RNNì„ ë§Œë“ ë‹¤. ì´ í´ë˜ìŠ¤ëŠ” ì²« ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ë¡œ ìˆœí™˜ ì¸µì˜ ê°ì²´ë¥¼ ì „ë‹¬ë°›ëŠ”ë‹¤.

ì „ë‹¬ë°›ì€ ìˆœí™˜ ì¸µìœ¼ë¡œ ìƒˆë¡œìš´ ë‘ ë²ˆì§¸ ê°ì²´ë¥¼ ìƒì„±í•œë‹¤.
ìˆœë°©í–¥, ì—­ë°©í–¥ 2ê°€ì§€ë¥¼ ë§Œë“¤ê³  ì²˜ë¦¬í•œë‹¤.


```python
inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.Bidirectional(layers.LSTM(16))(inputs)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=10,
                    validation_data=val_dataset)
```

ì´ ëª¨ë¸ì€(ì–‘ë°©í–¥ RNN)ì€ í‰ë²”í•œ LSTM ë§Œí¼ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤.  
ì´ìœ ëŠ” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë‹¤

> ëŒ€ë¶€ë¶„ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì€ ìˆœë°©í–¥ ì²˜ë¦¬ì—ì„œ ë‚˜íƒ€ë‚¨

ë™ì‹œì— ì‹œê°„ ë°˜ëŒ€ ìˆœì„œë¡œ ì²˜ë¦¬í•˜ëŠ” ì¸µ ë•Œë¬¸ì— ë„¤íŠ¸ì›Œí¬ ìš©ëŸ‰ì´ 2ë°°ê°€ ë˜ê³  í›¨ì”¬ ë” ì¼ì° ê³¼ëŒ€ì í•©ì´ ì‹œì‘ë©ë‹ˆë‹¤.

í•˜ì§€ë§Œ ì–‘ë°©í–¥ RNNì€ í…ìŠ¤íŠ¸ ë°ì´í„° ë˜ëŠ” ìˆœì„œê°€ ì¤‘ìš”í•œ(ì‚¬ìš© ìˆœì„œê°€ ì•„ë‹Œ?) ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ë°ì´í„°ì— ì˜ ë§ë‹¤.



### 10.4.4 ë” ë‚˜ì•„ê°€ì„œ
-----
ì˜¨ë„ ì˜ˆì¸¡ ë¬¸ì œì˜ ì„±ëŠ¥ì„ í–¥ìƒí•˜ê¸° ìœ„í•´ ì‹œë„í•´ ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì´ ë§ì´ ìˆë‹¤.

- ìŠ¤íƒœí‚¹í•œ ê° ìˆœí™˜ ì¸µì˜ ìœ ë‹› ê°œìˆ˜ì™€ ë“œë¡­ì•„ì›ƒì˜ ì–‘ì„ ì¡°ì •í•œë‹¤.
    ì§€ê¸ˆì€ ëŒ€ë¶€ë¶„ ì„ì˜ë¡œ í–ˆìœ¼ë‹ˆ ìµœì í™”ê°€ ëœ ë˜ì—ˆì„ ê²ƒ

- RMSprop ì˜µí‹°ë§ˆì´ì €ì˜ í•™ìŠµë¥ ì„ ì¡°ì •í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©
- ìˆœí™˜ ì¸µ ìœ„ì— ë†“ì„ íšŒê·€ ëª¨ë¸ì„ ìœ„í•´ í•˜ë‚˜ê°€ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ê°œì˜ Dense ì¸µì„ ìŒ“ëŠ”ë‹¤.
- ëª¨ë¸ì˜ ì…ë ¥ì„ ê°œì„ í•œë‹¤. ë” ê¸¸ê±°ë‚˜ ì§§ì€ ì‹œí€€ìŠ¤ë¥¼ í…ŒìŠ¤íŠ¸í•´ ë³´ê±°ë‚˜ ìƒ˜í”Œë§ ê°„ê²©(Sampling_rate)ë¥¼ ë³€ê²½ í˜¹ì€ íŠ¹ì„± ê³µí•™ ìˆ˜í–‰


> ì‘ê°€ëŠ” ì´ ë°ì´í„°ë¡œ 10% í–¥ìƒ ì‹œí‚¤ëŠ” ê²ƒì´ ìµœì„ ì´ë¼ ë§í•¨  
> ì´ìœ ëŠ” í•œ ì§€ì—­ì— ëŒ€í•œ ì¸¡ì •ê°’ë§Œ ì•Œì•„ì„œ...


ì´ ê¸€ì„ ë‹¤ ì½ê³  ìˆœí™˜ ì‹ ê²½ë§ì„ ì´í•´í–ˆë‹¤ë©´ ë„ˆëŠ” ì²œì¬ë‹ˆê¹Œ [ëŒ€íšŒ ì°¸ì—¬](https://dacon.io/competitions/official/236125/overview/description)í•´ë´ **ì°¸ê³ ë¡œ 7ì›” 25ì¼ ê¸°ì¤€ D-34ì„.**